\documentclass{article}
\input{preamble.tex}

\title{Estimation of a common covariance matrix for multiple classes with applications in meta- and discriminant analysis}
\author{
  Anders Ellern Bilgrau \\ \texttt{abilgrau@math.aau.dk} \and
  Poul Svante Eriksen \\ \texttt{svante@math.aau.dk} \and
  Martin B\o{}gsted \\ \texttt{mboegsted@dcm.aau.dk}
}

\begin{document}

<<initalize_knitr, echo=FALSE, results='hide', message=FALSE>>=
library("knitr")
options(width = 80)
opts_chunk$set(size = "footnotesize", fig.align = "center")
read_chunk("RCM.R")
@

<<initialize_script, echo=FALSE, results='hide', message=FALSE>>=
@

<<auxiliary_functions, echo=FALSE>>=
@

\maketitle
\phantomsection
\addcontentsline{toc}{section}{Abstract}
\begin{abstract}
We propose a hierarchical random effects model and estimators for a common covariance matrix in cases where multiple classes are present.
It is applicable where the classes are believed to share a common covariance matrix of interest obscured by class-dependent noise.
As such, it provides a basis for integrative or meta-analysis of covariance matrices where the classes are formed by datasets.
Our approach is inspired by traditional meta-analysis using random effects models but the model is also shown to be applicable as an intermediate between linear and quadratic discriminant analysis.
We derive the basic properties and estimators of the model and compare their properties.
Simple inference and interpretation of the introduced parameter measuring the inter-class homogeneity is suggested.
\medskip

\noindent \textbf{Keywords:} \textit{covariance estimation, meta-analysis, integrative analysis, network integration, gaussian graphical modeling, discriminant analysis}
\end{abstract}



\section{Introduction}
The fundamental problem in statistics of accurately and precisely estimating the covariance matrix (or its inverse) is notoriously difficult.
The usual bias-corrected maximum likelihood estimator (MLE), the sample covariance matrix, has long been known to perform poorly in general due to high variability \citep{Dempster1972}.
The sample covariance becomes increasingly ill-conditioned as the number of variables $p$ approaches the sample size $n$ and singular when $p$ exceeds $n$.
Because of its central statistical role the list of statistical methods and applications utilizing the estimated covariance matrix is exceedingly long.
Beside the many standard statistical methods such as principal component analysis (PCA), linear discriminant analysis (LDA), and quadratic discriminant analysis (QDA), examples of direct applications include gene and protein network analysis \citep{Butte2000}, spectroscopic imaging \citep{Lin2007}, functional magnetic resonance imaging (fMRI), financial forecasting, and many more.
Among this expanding list of applications is also an increasing number of high-dimensional applications and datasets publicly available at online repositories.

In high-dimensional datasets the number of features $p$ often far exceed the number of samples $n$.
Since the number of parameters increases quadratically in $p$ and the sample covariance become singular when $p > n$ a plethora of shrinkage and regularization estimators have been proposed to combat the accompanying problems by effectively increasing the degrees of freedom. These examples include the graphical LASSO and ridge estimation of the precision matrix \citep{Friedman2008, VanWieringen2014}.
Instead of attempting to derive still more sophisticated estimators we attempt to alleviate the problem from a different angle by using more available data and thus effectively increasing $n$.
While the high-dimensional extension to $p > n$ is important it is out of scope in this paper.
We restrict ourselves to the case where the total number of samples exceed $p$.
Hence, if $k$ classes or datasets are available with sample sizes $n_1, ..., n_k$, we consider the case where $p < \sum_{i=1}^k n_i$ while allowing $p$ to exceed $n_i$ for each individual class $i$.

As with all major groups of cancer, a large number of diffuse large B-cell lymphoma (DLBCL) genomic datasets are now publicly available online.
We wished to use these studies in combination with our own data to arrive at a good estimate of the covariance matrix whilst accounting for and assessing inter-study variation.
Although this work was motivated by gene-gene interaction networks in DLBCL where the covariance matrix is assumed to contain all information about the conditional dependencies of the genes, the methods are general and not limited to such genomic data.



\section{A random effects model for the covariance matrix}
The model below was motivated by ordinary meta-analysis.
Meta-analysis comes in various flavors corresponding to the assumption on the nature of the inter-study treatment effect.
Random-effects models (REM) in meta-analysis model the inter-study effects as random variables \cite{DerSimonian1986, Choi2003}.
In a vein similar to the ordinary meta-analysis approach, we think of the different studies as related but perturbed experiments and propose the following deceivingly simple random covariance model (RCM) of the observations.
Let $p$ be the number of features and $k$ the number of classes.
We model an observation $\vx$ from the $i$'th study as a $p$-dimensional zero-mean multivariate gaussian vector with covariance matrix realized from an inverse Wishart distribution, i.e.\ $\vx$ follows the hierarchical model
\begin{align}
\begin{split} \label{eq:RCM}
  \vSigma_i  &\sim \calW^{-1}_p\big((\nu - p - 1)\vSigma, \nu\big), \\
  \vx | \vSigma_i &\sim \calN_p(\vec{0}_p, \vSigma_i), \qquad i = 1, ..., k,
\end{split}
\end{align}
where $\calN_p(\vec{\mu},\vSigma_i)$ denotes a $p$-dimensional multivariate gaussian distribution with mean $\vec{\mu}$, positive definite (p.d.{}) covariance matrix $\vSigma_i$, and probability density function (pdf)
\begin{align*}
  f(\vec{x}| \vec{\mu}, \vSigma_i) =
  (2\pi)^{-\frac{p}{2}} |\vSigma_i|^{-\frac{1}{2}}
  \exp\!\left( -\frac{1}{2} (\vx - \vec{\mu})^\top \vSigma_i^{-1}(\vx - \vec{\mu}) \right).
\end{align*}
As seen, we use the generic notation $f(\cdot | \cdot)$ and $f(\cdot)$ for the conditional and unconditional pdf of random variables, respectively, throughout this paper.
Above, $\calW^{-1}_p(\vPsi, \nu)$ denotes a $p$-dimensional inverse Wishart distribution with degrees of freedom $\nu$, p.d. $p \times p$ scale matrix $\vPsi$, and pdf
\begin{align}
  \label{eq:wishartpdf}
  f(\vSigma_i) =
  \frac{ |\vPsi|^\frac{\nu}{2} }{
        2^\frac{\nu p}{2} \Gamma_p\!\left( \frac{\nu}{2} \right) }
        |\vSigma_i|^{-\frac{\nu+p+1}{2}}
  \exp\!\left( -\frac{1}{2} \tr\!\big(\vPsi\vSigma_i^{-1}\big) \right),
  \quad\nu > p - 1
\end{align}
where $\vSigma_i$ is p.d., $\nu > p - 1$, and $\Gamma_p$ is the multivariate generalization of the gamma function $\Gamma$ seen in Appendix \ref{sec:multigamma}.
While the inverse Wishart distribution is defined for all $\nu > p - 1$, the first order moment $(\nu - p - 1)^{-1}\vPsi$ exists only when $\nu > p + 1$.
With the reparameterization
\begin{align}
  \label{eq:reparameterization}
  \vPsi = (\nu - p - 1)\vSigma, \qquad \nu > p + 1,
\end{align}
of the inverse Wishart distribution of \eqref{eq:RCM}, the pdf of $\vSigma$ is given by
\begin{align}
  \label{eq:reparwishartpdf}
  f(\vSigma) =
  \frac{ |\nu-p-1|^\frac{\nu}{2} |\vPsi|^\frac{\nu}{2} }{
        2^\frac{\nu p}{2} \Gamma_p\!\left( \frac{\nu}{2} \right) }
        |\vSigma|^{-\frac{\nu+p+1}{2}}
  \exp\!\left( -\frac{\nu - p - 1}{2} \tr\!\big(\vPsi\vSigma^{-1}\big) \right),
\end{align}
and the common expected covariance matrix is
\begin{align}
  \label{eq:expcovar}
  \bbE[\vSigma_i] = \frac{\vPsi}{\nu-p-1} = \vSigma \quad\text{ for } \nu > p + 1.
\end{align}
Hence in the RCM given by \eqref{eq:RCM}, $\vSigma$ can be interpreted as a location-like parameter as it is the expected covariance matrix in each study.
The parameter $\nu$ inversely controls the inter-study or inter-class variation and can thus be considered an inter-study homogeneity parameter of the covariance structure.
A large $\nu$ corresponds to high study homogeneity and vice versa for small $\nu$.
This can be further seen as $\vSigma_i$ concentrates around $\vSigma$ for $\nu\to\infty$ which can be interpreted as the inter-study variation goes towards zero for increasing $\nu$.
Thus, the true underlying covariance matrix and the homogeneity parameters are the effects of interest to be estimated in this paper.

These basic properties of the RCM motivates the construction.
We note that while the reparameterization of \eqref{eq:RCM} has preferable interpretation, the likelihood is much more complex and often numerically unstable.
The reparameterization is especially problematic for $\nu$ near $p+1$ and indeed senseless when the expected covariance cease to exist for $p - 1 < \nu \leq p + 1$.
Therefore, we use the usual parameterization by $\vPsi$ in the fitting procedure and the remainder of this paper.





\subsection{The likelihood function}
Suppose $\vx_{i1}, \dots,\vx_{in_i}$ are $n_i$ i.i.d.\ observations from $i = 1,...,k$ independent studies from the model given in \eqref{eq:RCM}.
Let $\vX_i = (\vx_{i1}, \dots,\vx_{in_i})^\top$ be the $n_i \times p$ matrix of observations for the $i$'th study where rows correspond to samples and columns to variables.
By the independence assumptions, the log-likelihood for $\vPsi$ and $\nu$ is given by
\begin{align*}
  &\ell\!\left(\vPsi, \nu \big|\vX_1, ..., \vX_k  \right)
  = \log f\!\left(\vX_1, ..., \vX_k \big| \vPsi, \nu \right) \\
  &= \log\!\int
             f(\vX_1, ...,\vX_k |
               \vSigma_1, ..., \vSigma_k, \vPsi, \nu)
             f(\vSigma_1, ..., \vSigma_k | \vPsi, \nu)
             d\vSigma_1 \cdots d\vSigma_k \\
  &= \log\!\int
               \prod_{i=1}^k
               f(\vX_i | \vSigma_i)
               f(\vSigma_i | \vPsi, \nu)
               d\vSigma_1 \cdots d\vSigma_k \\
  &= \sum_{i=1}^k \log\!\int
               f(\vX_i | \vSigma_i)
               f(\vSigma_i | \vPsi, \nu)
               d\vSigma_i.
\end{align*}
Since the inverse Wishart distribution is conjugate to the multivariate gaussian distribution the integral, of which the integrand forms a gaussian-inverse-Wishart distribution, can be evaluated. Hence $\vSigma_i$ can be marginalized out, cf.\ \eqref{eq:marg1} in Appendix \ref{sec:marginalization}, and we arrive at the following expression for the log-likelihood function,
\small
\begin{align}
  &\ell\!\left(\vPsi, \nu \big| \vX_1, ..., \vX_k \right) %\notag\\
  = \log\prod_{i=1}^k
    \frac{\big|\vPsi\big|^\frac{\nu}{2} \Gamma_p\!\left(\frac{\nu+n_i}{2}\right)}
         {\pi^\frac{n_i p}{2} \big|\vPsi +\vX_i^\top\vX_i\big|^\frac{\nu+n_i}{2}
          \Gamma_p\!\left(\frac{\nu}{2}\right)}          \notag\\
  &= c + \sum_{i=1}^k \!\bigg[
            \frac{\nu}{2}  \log\big|\vPsi\big|
            + \log\Gamma_p\!\left(\frac{\nu + n_i}{2}\right)
   % \notag\\ &\hspace{1.5cm}
       - \frac{\nu + n_i}{2}\log\big| \vPsi +\vX_i^\top\vX_i \big|
            - \log\Gamma_p\!\left(\frac{\nu}{2}\right)
            \!\bigg]\!,
    \label{eq:loglik}
\end{align}
\normalsize
where constant terms are abbreviated by $c$.
As should be expected, the scatter matrix $\vec{S}_i =\vX_i^\top\vX_i$ and study sample size $n_i$ are sufficient statistics for each study.
Note that $\vS_i$ is conditionally Wishart distributed, $\vS_i|\vSigma_i \sim \calW(\vSigma_i, n_i)$, by construction.

As stated in the following two propositions, the likelihood is not log-concave in general. However it is log-concave as a function of $\nu$.

\begin{restatable}[Non-concavity in $\vPsi$]{proposition}{propositionNonConcavityInPsi}
  \label{prop:nonconcavityinpsi}
  For fixed $\nu$, the log-likelihood function \eqref{eq:loglik} is not
  concave in $\vPsi$ (and hence not in general).
\end{restatable}

\begin{restatable}[Concavity in $\nu$]{proposition}{propositionConcavityInNu}
  \label{prop:concavityinnu}
  For fixed $\vPsi \succeq 0$, the log-likelihood function \eqref{eq:loglik}
  is concave in $\nu$.
\end{restatable}

\noindent The proofs of these propositions has been deferred to Appendix \ref{sec:concaveloglik}.
While the likelihood is not concave in $\vPsi$ we are able to show that the existence and uniqueness of a global maximum in $\vPsi$.

\begin{restatable}[Existence and uniqueness of maximum]{proposition}{propositionUniqueMax}
\label{prop:uniquemax}
The log-likelihood \eqref{eq:loglik} has a unique maximum in $\vPsi$ for fixed $\nu$ and $n_\bullet = \sum_{a=1}^k n_a \geq p$.
\end{restatable}

\noindent This result follows from by the following two lemmas.

\begin{restatable}{lemma}{lemmaOne}
\label{lem:elltominusinfty}
  If $\nu$ is fixed and $n_\bullet = \sum_{a=1}^k n_a \geq p$ then the log-likelihood $\ell(\vPsi) \to -\infty$ whenever $\exists \lambda_i : \lambda_i \to 0 \vee \lambda_i \to \infty$, where $\lambda_i$ is a eigenvalue of $\vPsi$.
\end{restatable}

\begin{restatable}{lemma}{lemmaTwo}
\label{lem:negativesdefinite}
If $n_\bullet > p$ and $\nu$ is fixed then the hessian of the log-likelihood \eqref{eq:loglik} is negative definite in all stationary points.
\end{restatable}

\noindent Lemma \ref{lem:negativesdefinite} tells that the Hessian in any stationary point (i.e.\ where $\frac{\partial\ell}{\partial\vPsi} = \vec{0}$) is negative definite, and hence every stationary point is a local maxima.
This combined with the observation that $\ell(\vPsi) \to -\infty$ whenever an eigenvalue $\lambda_i \to 0$ or $\lambda_i \to \infty$ of Lemma \ref{lem:elltominusinfty} implies Proposition \ref{prop:uniquemax} and the existence and uniqueness of global maximum.
Again, proofs for Proposition \ref{prop:uniquemax} and the accompanying lemmas can be found in Appendix \ref{sec:negativedefinite}.

We have also derived MLE estimators using a first order approximation. However, as simulation experiments later show, this estimator performed quite poorly and has therefore been deferred to Appendix \ref{sec:amle}.



\subsection{Moment estimator}
Within the RCM framework, we show that the pooled empirical covariance matrix can be viewed as a moment estimator of $\vSigma$.

By the assumptions the first and second moment of the $j$'th observation in the $i$'th study, $\vx_{ij}$, is given by $\bbE[\vx_{ij}] = \vec{0}_p$ and
\begin{align*}
  \bbE[\vx_{ij}\vx_{ij}^\top]
    &= \bbE\!\left[ \bbE[ \vx_{ij}\vx_{ij}^\top | \vSigma_i ]\right]
    = \bbE[\vSigma_i]
    = \frac{\vPsi}{\nu - p - 1}
    = \vSigma.
\end{align*}
for all $j = 1, ..., n_i$ and $i = 1, ..., k$. This suggests the estimator
\begin{align}
  \label{eq:pooledest}
  \hvPsi_\text{pool}
  = (\nu - p - 1)\frac{\sum_{i = 1}^k \vS_i}{\sum_{i = 1}^k n_i},
\end{align}
from which
\begin{align*}
  \hvSigma_\text{pool}
  = \frac{\sum_{i = 1}^k \vS_i}{\sum_{i = 1}^k n_i}, \qquad \nu > p + 1
\end{align*}
by e.g.\ plugging the estimate $\hvPsi_\text{pool}$ into \eqref{eq:reparameterization} or \eqref{eq:expcovar}.
This is the well-known pooled empirical covariance matrix.



\subsection{Maximization using the EM algorithm}
Here the updating scheme of the expectation-maximization (EM) algorithm \citep{Dempster1977} for fixed $\nu$ is derived.
Suppose $\hvPsi_{(t)}$ is a current estimate of $\vPsi$ and that $\vS_i$ is the empirical scatter matrix.
We now compute the expectation step of the EM-algorithm.

From \eqref{eq:RCM} we equivalently have that,
\begin{align*}
  \vSigma_i          &\sim \calW^{-1}_p\big(\vPsi, \nu\big), \\
  \vS_i | \vSigma_i  &\sim \calW_p(\vSigma_i, n_i) \quad \text{ for } i = 1, ..., k.
\end{align*}
Let $\vDelta_i = \vSigma_i^{-1}$ be the precision matrix and let $\vTheta = \vPsi^{-1}$, then
\begin{align}
  \vDelta_i
  &\sim \calW_p\big(\vTheta, \nu\big),
  \notag\\
  \vS_i | \vDelta_i
  &\sim \calW_p( \vDelta_i^{-1}, n_i).
  \label{eq:precisiondensity}
\end{align}
From the conjugacy of the inverse Wishart and the Wishart distribution, we have the posterior distribution of the precision matrix,
\begin{align*}
    \vDelta_i | \vS_i
    &\sim \calW_p\!\Big( \big(\vTheta^{-1} + \vS_i\big)^{-1}, n_i + \nu\Big).
\end{align*}
Hence, the expectation step is given by
\begin{align*}
  \bbE[\vDelta_i |\vS_i] = (n_i + \nu)\big(\vTheta^{-1} + \vS_i\big)^{-1}.
\end{align*}
The maximization step, in which the log-likelihood $\ell(\vTheta|\vDelta_1, ..., \vDelta_k)$ is maximized, yields the estimate
\begin{align*}
 \hat{\vTheta} = \frac{1}{k\nu}\sum_{i = 1}^k \vDelta_i,
\end{align*}
which is the mean of the scaled precision matrices $\frac{1}{\nu}\vDelta_i$.
The derivation of this estimate can be seen in Appendix \ref{sec:precisionloglik}.
The above yield the updating scheme
\begin{align}
  \label{eq:em}
  \hvTheta_{(t+1)}
  = \frac{1}{k\nu}\sum_{i = 1}^k
    (n_i + \nu)\left(\hvTheta_{(t)}^{-1} + \vS_i\right)^{-1}
\end{align}
for $\vTheta$.
The connection to the likelihood equation \eqref{eq:firstordderivloglik} is immediately seen.
We denote the inverse of the estimate obtained by repeated iteration of \eqref{eq:em} by $\hvPsi_\text{EM}$.


\subsection{Estimation procedure}
We propose a procedure alternating between estimating $\nu$ and $\vPsi$ while keeping the other fixed.
Given parameters $\hat{\nu}_{(t)}$ and $\hvPsi_{(t)}$ at iteration $t$, we estimate $\hvPsi_{(t+1)}$ using fixed $\hat{\nu}_{(t)}$. Subsequently, we find $\hat{\nu}_{(t+1)}$ by a standard one-dimensional numerical optimization procedure using the fixed $\hvPsi_{(t+1)}$.
This coordinate ascent approach is repeated until convergence.
More precisely, in pseudo-code, we propose the algorithm seen in Algorithm \ref{alg:RCM}.
\begin{algorithm}[tb]
\caption{Pseudo-code for the RCM estimation procedure}\label{alg:RCM}
\begin{algorithmic}[1]
\Procedure{RCM coordinate ascent}{}
\State \algorithmicrequire{
\State \emph{Sufficient data:} $(\vS_1, n_1), ..., (\vS_k, n_k)$
\State \emph{Initial parameters:} $\hvPsi_{(0)}, \hat{\nu}_{(0)}$
\State \emph{Convergence criterion:} $\varepsilon > 0$
}
\State \algorithmicensure{
\State \emph{Parameters:} $\hvPsi, \hat{\nu}$
}
\State
\State \emph{Initialize}: $l_{(0)} \gets \ell(\hvPsi_{(0)}, \hat{\nu}_{(0)})$
\For {$t = 1, 2, 3, ...$}
  \State $\hvPsi_{(t)} \gets U\!\left(\hvPsi_{(t-1)}, \hat{\nu}_{(t-1)}\right)$
  \State $\hat{\nu}_{(t)} \gets \argmax_\nu \ell\!\left(\hvPsi_{(t)}, \nu\right)$
  \State $l_{(t)} \gets \ell\!\left(\hvPsi_{(t)}, \hat{\nu}_{(t)}\right)$
    \If {$l_{(t)} - l_{(t-1)} < \varepsilon$}
      \State \Return $\left(\hvPsi_{(t)}, \nu_{(t)}\right)$
    \EndIf
 \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}
The update function $U$ in the algorithm is defined by the derived estimators above.
That is, equations \eqref{eq:pooledest}, \eqref{eq:mle}, or \eqref{eq:em} define $U$ to be the pooled, approximate MLE, and EM estimate respectively.

The procedure using the EM step utilizes the results about the RCM log-likelihood and thus provides a guarantee of convergence along with the advantage of a very simple implementation.
However, the obvious disadvantage is that the identified maxima might be a saddle-point when considering the log-likelihood jointly in $(\vPsi, \nu)$.



\subsection[Interpretation and inference of nu]{Interpretation and inference}

\subsubsection*{Test for no class heterogeneity}
By the RCM construction, $\nu$ parameterizes an inter-class variance where the size of $\nu$ corresponds to the homogeneity between the classes or studies.
Large $\nu$'s yield high study homogeneity and small $\nu$'s yield low homogeneity.
Thus it is of interest to test if the estimated homogeneity $\hat{\nu}$ is extreme under the null-hypothesis of no heterogeneity (i.e.\ infinite homogeneity).
I.e.\ we wish to test the hypothesis $H_0: \nu = \infty$ which is equivalent to
\begin{align*}
  H_0: \vSigma_1 = ... = \vSigma_k = \vSigma.
\end{align*}
The two are equivalent since the sampling of the covariance matrix from the inverse Wishart distribution becomes deterministic when $\nu = \infty$. Testing this hypothesis can therefore also be interpreted as testing if the data is adequately explained when leaving out the hierarchical structure.

The distribution of $\hat{\nu}$ under the null hypothesis, however, is not tractable. In practice, under $H_0$ or when $\nu$ is very large the estimated $\hat{\nu}_\text{obs}$ will be finite as the intra-study variance dominate the total variance.
However, the distribution of $\hat{\nu}$ under $H_0$ is not easily characterized.
We propose to approximate the distribution of $\hat{\nu}$ under $H_0$ by simulation.
To do this, the model is simply fitted on datasets generated under $H_0$ a large number of times $N$ to get $\hat{\nu}_0^{(1)}, ..., \hat{\nu}_0^{(N)}$. As \textit{small} values of $\hat{\nu}$ are critical for $H_0$ approximate acceptance regions can be constructed from $\hat{\nu}_0^{(j)}, j = 1,...,N$. Likewise, an approximation of the $P$-value testing $H_0$ can be obtained by
\begin{align*}
  P = \frac{1}{N+1} \left( 1 + \sum_{j=1}^N \bbOne\!\Big[\hat{\nu}_0^{(j)} < \hat{\nu}_\text{obs}\Big] \right)
\end{align*}
where $\bbOne[\cdot ]$ is the indicator function. This is approximately the fraction of $\hat{\nu}_0$'s smaller than $\hat{\nu}_\text{obs}$.


\subsubsection*{Intra-class correlation coefficient analogue}
We also introduce a descriptive statistic analogous to the intra-class correlation coefficient (ICC) \citep[See e.g.]{Stanish1983} well known from ordinary meta-analysis to better determine what constitute large values of $\nu$.
We define a modified ICC given by
\begin{align}
  \text{ICC}(\nu)
  = \frac{1}{\nu-p}.
  \label{eq:ICCexprs}
\end{align}
This definition is motivated by considering the ratio of the between-study variation and the total variation of the samples as seen below.
Consider observations from \eqref{eq:RCM}. We (temporarily) abuse our own notation and let
\begin{align*}
  \vSigma \sim \calW_p^{-1}\!( \vPsi, \nu) \quad \text{ and }\quad
  \vS | \vSigma \sim \calW_p\!(\vSigma, 1),
\end{align*}
where we only consider a single observation $(n = 1)$.
Further let $\vS = (S_{ij})_{p\times p}$, $\vSigma = (\Sigma_{ij})_{p \times p}$, and $\vPsi = (\Psi_{ij})_{p \times p}$.
To compute the ICC, we are thus interested in the ratio of the quantities $\var(\Sigma_{ij})$ and $\var(S_{ij})$ corresponding to the between-study and total variation of the covariance between $i$ and $j$, respectively.
That is, the ICC is the proportion of the total variance between studies,
\begin{align}
  \text{ICC}(\nu)
  = \frac{\var(\Sigma_{ij})}
         {\var(S_{ij})}
  = \frac{\var(\Sigma_{ij})}
         {\var(\Sigma_{ij}) + \bbE[ \var(S_{ij}|\vSigma) ]},
  \label{eq:ICC}
\end{align}
where the second equality is obtained by $\bbE[S_{ij}|\vSigma] = \Sigma_{ij}$ and the law of total variation.
This equality agrees with the usual ICC as $\bbE[\var(S_{ij}|\Sigma_{ij})]$ can be interpreted as the (expected) within-study variation.
Using the conditional variance $\var(S_{ij}|\vSigma) = \Sigma_{ij}^2 + \Sigma_{ii} \Sigma_{jj}$ the needed quantities can be found.
To compute an expression for \eqref{eq:ICC} we need to consider the third-order moments of the observations.
From the model,
\begin{align}
  \label{eq:invwishcovar}
  \cov(\Sigma_{ij}, \Sigma_{kl})
  &= \frac{2\Psi_{ij}\Psi_{kl}+ (\nu-p-1)\big(\Psi_{ik}\Psi_{jl} + \Psi_{il}\Psi_{kj}\big)}
          {(\nu-p)(\nu-p-1)^2(\nu-p-3)}, \quad\nu > p +3,
  \intertext{implying that}
  \var(\Sigma_{ij})
  &= \cov(\Sigma_{ij}, \Sigma_{ij})
  \label{eq:invwishvar}
  = \frac{(\nu-p+1)\Psi_{ij}^2 + (\nu-p-1)\Psi_{ii}\Psi_{jj}}
          {(\nu-p)(\nu-p-1)^2(\nu-p-3)}
\end{align}
by known results of the inverse Wishart distribution, cf.\ \citep{Cook2011, Rosen1988}.
We continue with the conditional variance of $S_{ij} | \vSigma$ in the denominator of \eqref{eq:ICC},
\small
\begin{align}
  \bbE\big[\var(S_{ij}|\Sigma_{ij})\big]
  &= \var(\Sigma_{ij}) + \bbE[\Sigma_{ij}]^2
    + \cov(\Sigma_{ii}, \Sigma_{jj})
    + \bbE[\Sigma_{ii}]\bbE[\Sigma_{jj}]  \notag\\
  &= \var(\Sigma_{ij}) + \cov(\Sigma_{ii}, \Sigma_{jj})
    + (\nu-p-1)^{-2}(\Psi_{ij}^2 + \Psi_{ii}\Psi_{jj}).
    \label{eq:varSij}
\end{align}
\normalsize
An expression of $\var(S_{ij})$ in terms of the elements of $\vPsi$ can then found by substituting \eqref{eq:invwishcovar} and \eqref{eq:invwishvar} into \eqref{eq:varSij} and by extension an expression for the ICC \eqref{eq:ICC} can be obtained.
We omit this tedious calculation which can be verified to yield \eqref{eq:ICCexprs} above.
Naturally enough, the ICC depends only on $\nu$.
A straight-forward plug-in estimator $\widehat{\text{ICC}}(\nu)$ of the ICC of some gene-gene interaction is then $\text{ICC}(\hat{\nu})$.

Though $v > p + 3$ is required for the variances to exist, it is clear that
\begin{align*}
  \lim_{\nu \to (p+1)^+} \text{ICC}(\nu) = 1 \quad \text{ and } \quad
  \lim_{\nu \to \infty^-} \text{ICC}(\nu) = 0,
\end{align*}
as should be expected.




<<numerical_experiment, echo=FALSE, results='hide'>>=
@
\section{Assessment of the estimation procedures}
To assess the precision and stability of the estimation procedure we generated data from the hierarchical model \eqref{eq:RCM} for $p = \Sexpr{par.ne[["p"]]}$ variables and $k = \Sexpr{par.ne[["k"]]}$ studies each with an equal number of observations, $n = n_1 = n_2 = n_3$. We chose the parameters $\nu = \Sexpr{par.ne[["nu"]]}$ and
\begin{align*}
\vSigma =
  \begin{bmatrix}
     1 & 0.5 & 0.5 & \cdots\\
     0.5 & 1 & 0.5 & \cdots\\
     0.5 & 0.5 & 1 & \cdots\\
     \vdots & \vdots & \vdots & \ddots
  \end{bmatrix}.
\end{align*}
The number of observations in each study $n$ was varied in the range $\Sexpr{paste0("[", min(par.ne$n.obs), ",", max(par.ne$n.obs), "]")}$.

We measure the precision of the estimated values against the expected covariance matrix given by \eqref{eq:expcovar}.
Let $\hvPsi$ and $\hat{\nu}$ be the estimates obtained using the moment, EM, or approximate MLE (defined in Appendix \ref{sec:amle}) approach as described. We benchmark the proposed estimators against the known truth. The benchmarking measure used is the following weighted sum of squared errors,
\begin{align*}
  \text{SSE}(\hvPsi) = \sum_{i \leq j} \frac{(\hat{\Psi}_{ij}- \Psi_{ij})^2}{\var(\Psi_{ij})}
\end{align*}
where
\begin{align*}
 \var(\Psi_{ij}) = n(\Psi_{ij}^2 + \Psi_{ii}\Psi_{jj}).
\end{align*}

For each $n = \Sexpr{par.ne$n.obs}$, the weighted sum of squared errors for each estimator, $\text{SSE}(\hvPsi)$, where computed for $\Sexpr{par.ne[["n.sims"]]}$ datasets and the average of these values are seen in Figure \ref{fig:numerical_experiment_plot} as function of the number of samples in each dataset $n_i$.

<<numerical_experiment_plot, echo=FALSE, results='markup', fig.pos="tb", fig.height=7/2, fig.width=7, out.width = ".8\\linewidth", fig.scap = "", fig.cap=paste0("\\it The estimated mean weigthed sum of squared errors (MSSE) based on ", par.ne$n.sims, " simulations, as a function of the number of samples $n_i$ in each study.")>>=
@

\noindent Unsurprisingly, the proper EM estimation outperforms the simple alternative estimates.
We see that the EM estimation is superior to that of the approximate MLE and moment estimators.





\subsection{Implementation and availability}
The algorithm along with the different estimators are implemented in the statistical programming language R \cite{R} with workhorse functions in C++ using packages Rcpp and RcppArmadillo \citep{Eddelbuettel2011, RcppArmadillo}.
They are incorporated in the open-source R-package \texttt{correlateR} freely available for forking and editing at \url{http://github.com/AEBilgrau/correlateR}.
See the documentation here for further details.
This document was prepared with \texttt{knitr} \citep{Xie2013} and LaTeX.



\section{Applications}
<<dlbcl_analysis, echo=FALSE, results='hide', message=FALSE>>=
@

\subsection{Meta-analysis of DLBCL}
Diffuse large B-cell lymphoma (DLBCL) is an aggressive cancer subtype accounting for approximately $31 \%$ of all non-Hodgkin's lymphoma (NHL) \citep{Project1997} which itself constitutes about $90 \%$ of all lymphomas.


\subsubsection*{Data and preprocessing}
A large amount of DLBCL datasets are now available online at the NCBI (National Center for Biotechnology Information) Gene Expression Omnibus (GEO) website.
We downloaded 10 DLBCL large-scale gene expression studies and preprocessed these using the custom brainarray chip definition files (CDF) \citep{Dai2005} and RMA-normalization using the R-package \texttt{affy} \citep{affy}.
The corresponding GEO-accession numbers are \Sexpr{grep("XXX", base::unique(studies[,"GSE"]), invert = TRUE, value = TRUE)} based on various microarray platforms.
The downloaded data together with our own (GSE56315 \citep{DykaerBoegsted2015}) yields a total of \Sexpr{rowSums(dlbcl.dims)["Samples"]} samples with study sizes in the range \Sexpr{paste(range(dlbcl.dims["Samples",]), collapse = "-")}.
The summarization using brainarray CDFs to Ensembl gene identifiers facilitates cross-platform integration.

After the RMA normalization and summarization, the data was brought to a common scale by quantile normalizing all data to the common cumulative distribution function of all arrays.
Lastly, the datasets were reduced to the total of \Sexpr{dim(gep[[1]])["Features"]} common features represented in all studies and array platforms.

\subsubsection*{Meta-analysis}
We wanted to perform a co-expression network (i.e.\ weighted correlation network) analysis integrating all datasets.
For each dataset the scatter matrix $\vS_i$ of the top \Sexpr{dlbcl.par$top.n} most variable genes (as measured by the pooled variance across studies) was computed as the sufficient statistics along with the number of samples.
Hence, we investigate \Sexpr{format(dlbcl.par$top.n*(dlbcl.par$top.n + 1)/2, big.mark = ",")} pairwise interactions.

The parameters of the RCM model was estimated using the EM algorithm and yielded the $\Sexpr{dlbcl.par$top.n} \times \Sexpr{dlbcl.par$top.n}$ matrix $\hvPsi$ and $\hat{\nu} = \Sexpr{round(dlbcl.rcm[["nu"]], 2)}$.
From these, $\hvSigma = (\hat{\nu}-p-1)^{-1}\hvPsi$ was computed and subsequently scaled to the corresponding correlation matrix $\hat{\vec{R}}$.

The $\hat{\nu}$ yield a surprisingly low estimated ICC of $\Sexpr{round(1/(dlbcl.rcm[["nu"]] - dlbcl.par[["top.n"]]), 4)}$.
Hence by the RCM, only $\Sexpr{round(1/(dlbcl.rcm[["nu"]] - dlbcl.par[["top.n"]]), 4)*100} \%$ of the variability of the estimated covariances is between-studies.
Possible explanations for this could be comparatively high within-study variability or indeed a high study homogeneity.


<<dlbcl_mappings, echo=FALSE, results='hide'>>=
@

<<dlbcl_clustering, echo=FALSE, results='hide'>>=
@

<<dlbcl_plot_2, echo = FALSE, message=FALSE, results='hide'>>=
@

\fig{\Sexpr{dlbcl_plot_2}}{0.8\textwidth}{\it Top left: dendrograms of the hierarchical clustering, the identified modules, and the correlation matrix. Top right: the correlation network as laid out by the Fruchterman-Reingold algorithm using the edge-weights. The nodes are colored after the identified modules with sizes proportional to the node connectivity (defined by row sums of $\hat{\vec{R}}$). The opacity of the edges are drawn as function of the edge weight. Bottom: A Hierarchical edge bundling representation of the network. Negative and positive correlations are colored blue and red, respectively.
}

We have thus estimated a common correlation matrix $\hat{\vec{R}}$ across all studies to which standard correlation network analyses can be employed.
To identify sub-graphs, we used agglomerative hierarchical clustering with \Sexpr{capitalize(dlbcl.clust$method)}-linkage on the distance measure defined by 1 minus the absolute value of the correlation.
The tree was arbitrarily pruned at a height which yielded \Sexpr{length(unique(dlbcl.modules))} sub-graphs (or, modules) colored and named by colors.
Figure \ref{\Sexpr{dlbcl_plot_2}} shows these results and Table \ref{tab:dlbcl_mod_tab} shows the genes within each module.

Next, the modules were screened for biological relevance using GO (Gene Ontology) enrichment analysis.
Table \ref{tab:GO_tabs} in Appendix \ref{sec:sig_go_term} show the significant GO-terms at significance level \Sexpr{dlbcl.par$go.alpha.level} for each module.
As seen, the GO-terms all appear highly relevant to the pathology of DLBCL.

<<dlbcl_go_analysis, echo=FALSE, results='hide'>>=
@

<<dlbcl_mod_tab, echo=FALSE, results='asis'>>=
@

Lastly, we checked if the identified modules were prognostic for overall survival (OS) in the CHOP and R-CHOP-treated cohorts of the LLMPP dataset (GSE10846).
To do this, the eigengene \cite{Horvath2011} for each module was computed and a multiple Cox proportional hazards model for OS was fitted with the module eigengenes as covariates.
For the most interesting modules, the Kaplan-Meier estimates were computed for groups arising when dichotomizing the values of corresponding eigengene as above or below the median value.
These results are seen in Figure \ref{fig:survival_analysis}.


<<survival_analysis, echo=FALSE, results='hide', fig.height=2.5*7*0.7, fig.width=2*7*0.7, fig.pos="tb", fig.scap = "", fig.cap="\\it The top row shows $95\\%$ and $99\\%$ confidence intervals for the estimate of the Hazard ratio in the multiple Cox proportional hazards model. The bottom two rows show Kaplan-Meier estimates of the dichotomizing eigengenes.">>=
@

From the survival analysis, the yellow module appeared particular interesting.
Therefore a manual screening of the 16 genes within the yellow module was performed.
Products CHI3L1 , CHIT1, and LYZ are related to chitin degradation and suggests activated immune system response and inflammation.
Enzymes related to chitin degradation can possibly originate from macrophages as CHIT1 is expressed by activated macrophages.
The inflammation and modulated activity of the immune system are further suggested by genes ORM1, PLA2G2D, PLA2G7, IL18, IGSF6.
CHIT3L1 (also known as YKL40) has been linked to the AKT anti-apoptotic signaling pathway in glioblastoma\cite{Francescone2011}.
Some of the remaining, MMP9, ALDH1A1, PTGDS, ADAMDEC1, HSD11B1, APOC1, CYP27B1, are involved in metalloproteinase degradation and lipid metabolism.
MMP9 is known have a central role in proliferation, migration, differentiation, angiogenesis, apoptosis and host defenses.
Numerous studies have linked altered MMP expression in different human cancers with poor disease prognosis where up-regulation of MMPs are associated with enhanced cancer cell invasion.
ADAMDEC1 is thought to have a central role in dendrite cell functions and their interactions with germinal center T cells.

The manual screening and GO-analysis results further corroborates that the identified modules are biologically meaningful and that RCM provides a reasonable estimate of the covariance.



\subsection{Supervised classification}
<<discriminant_analysis, echo=FALSE, message=FALSE, results='hide'>>=
@

\noindent Another application of the RCM is discriminant analysis.
As seen below, the estimates obtained can be utilized in supervised learning as a intermediate case between linear discriminant analysis (LDA) and quadratic discriminant analysis (QDA).

Suppose $Y$ is a random variable denoting the classes $1, ..., k$ and suppose $\vx$ is a random vector of the explanatory variables.
Recall that LDA and QDA finds the class $y$ by maximizing
\begin{align*}
  P(Y = y | \vX = \vx) =
    \frac{\pi_y f(\vx | Y = y)}
         {\sum_{y' = 1}^k \pi_{y'} f(\vx | Y = y')}
\end{align*}
where $\vx | Y = y$ is assumed to be $p$-dimensional gaussian distributed, i.e.\
\begin{align*}
  \vX | Y = y \sim \calN_p(\vec{\mu}_y, \vSigma_y).
\end{align*}
LDA differs from QDA by the additional assumption that $\vSigma = \vSigma_y$ for all classes $y$.
An intermediate classifier of LDA and QDA can thus be constructed by assuming the $\vSigma_y$'s are inverse Wishart distributed as in \eqref{eq:RCM}, i.e., $\vSigma_y \sim \calW^{-1}_p\!\big((\nu-p-1)\vSigma, \nu\big)$, and using the estimates of a common expected $\vPsi$ discussed above.
The hierarchical discriminant analysis (HDA) is thus straight-forward to implement given that
\begin{align*}
f(\vx | Y = y)
  &= \int f(\vx| \vSigma, Y = y) f(\vSigma | Y = y) d\vSigma \\
  &= \frac{ \big|\vPsi\big|^{\frac{\nu}{2}} \Gamma_p\!\left(\frac{\nu + 1}{2}\right) }
          { \pi^{-\frac{n}{2}}
            \big|\vPsi + (\vx-\vec{\mu}_y)(\vx-\vec{\mu}_y)^\top\big|^{\frac{\nu + 1}{2}}
            \Gamma_p\!\left(\frac{\nu}{2}\right)},
\end{align*}
analogous to the derivation in Appendix \ref{sec:marginalization}. The matrix determinant lemma, $|\vec{A} + \vec{u}\vPsi^\top| = (1 + \vPsi^\top\vec{A}\vec{u})|\vec{A}|$, can then be applied to simplify the expression and speed up the computations \citep{Ding2007}.
We note also that HDA generalizes the LDA in some sense to have a multivariate $t$-like distribution.
The above become a multivariate $t$-distribution when the sample sizes of each class is $1$.
Multivariate $t$-distributions have before been considered for discriminant analysis, cf.\ \citet{Andrews2011}.


\subsubsection*{Benchmarking of HDA}
We designed four different scenarios to test and identify where HDA can be expected to perform similarly, better, or worse compared to LDA and QDA as gauged by the misclassification risk. The simulation experiment was inspired by the one seen in \citet{Friedman1989}.

In all four scenarios, we generated for $p = \Sexpr{par.xda$p.dims}$ a training dataset of a total of $n = \Sexpr{par.xda[["n.obs"]]}$ observations belonging to $k = \Sexpr{par.xda[["K"]]}$ classes.
First, class labels were generated from a multinomial distribution with equal probabilities for each class, $\pi_1 = \pi_2 = \pi_3 = 1/3$.
Hence, in each simulation round $\Sexpr{round(par.xda[["n.obs"]]/par.xda[["K"]], 2)}$ observations were expected in each class.
Conditional on the class the observations where drawn i.i.d.\ from a multivariate gaussian distribution, i.e.\ $x_{i}|K = k \sim \calN_p(\vmu_k, \vSigma_k)$.
The four scenarios consists of different choices of covariance matrices $\vSigma_k$ and mean values $\vmu_k$ for each class.

The \Sexpr{par.xda[["K"]]} covariance matrices were chosen to either be (a) equal and spherical, (b) unequal and spherical, (c) equal and highly elliptical, and (d) unequal and highly elliptical.
In scenario (a), $\vSigma_1 = \vSigma_2 = \vSigma_3 = \vI$.
In scenario (b), $\vSigma_k = k \vI$.
In scenario (c), the covariance matrices equal, $\vSigma_1 = \vSigma_2 = \vSigma_3$, and chosen such that the square root of the $d$ eigenvalues are equidistant on the interval 10 to 1 and a randomly (uniformly) oriented orthonormal basis is used for all components \citep{Friedman1989}.
In scenario (d), the eigenvalues are chosen as in scenario (c) for all $\vSigma_k$ but the orientation of the orthonormal basis of eigenvectors differs.

In all scenarios expect (c) the mean values were chosen so $\vec{\mu}_1 = \vec{0}, \vec{\mu}_2 = 3\vec{e}_1,$ and $\vec{\mu}_3 = 4\vec{e}_2$ where $\vec{e}_j$ denotes the $j$'th basis-vector.
In scenario (c), $\vec{\mu}_1 = \vec{0}$ and the remaining mean values are chosen such that the differences project mainly onto the low-variance subspace

Using the described parameters a training and validation set each of $\Sexpr{par.xda[["n.obs"]]}$ and $\Sexpr{par.xda[["n.obs.valid"]]}$ observations, respectively, were generated.
For each method the classifier was trained on the training data followed by classification of the validation data and computation of the misclassification risk.
Each simulation setup was repeated $\Sexpr{par.xda[["n.runs"]]}$ times and the mean and standard deviation of the misclassification risks were computed.
Table \ref{HDA_tab} shows the results of the simulation experiments.

\providecommand{\red}{\color{red}}
\definecolor{mygreen}{rgb}{0,0.6,0}
\providecommand{\green}{\color{mygreen}}
<<hda_table_res, echo=FALSE, results='asis'>>=
@

\noindent In order to be able to evaluate $P(Y = y | \vX = \vx)$ in cases where the sample covariance matrix is not invertible, a small constant was added to the diagonal to allow for stable inversion similar to \citet{Friedman1989}. In the given setup, this modification was only necessary for QDA in the low-dimensional cases.

In the equal and spherical case (a), HDA yields almost identical results to LDA, which both unsurprisingly outperform QDA.
Also perhaps expected, the difference between LDA and QDA is less prominent for low dimensional spaces.
The same holds true for the unequal and spherical case (b) and (c).

Most interestingly, HDA is seen to always perform at least as good as LDA in all scenarios.
HDA is also consistently superior for the large dimensional tests.
The largest gain from HDA to LDA was seen in the high-dimensional scenario (c).

This demonstrates HDA as a potentially useful addition to the discriminant analysis toolbox.




\section{Concluding remarks}
This article provides a basic framework for modeling a common covariance structure across multiple classes or datasets.
The straight-forward approaches, of using the mean or pooled covariance matrix, are seen as moment estimators of the model and the estimate using the EM algorithm is shown to be superior to the simple alternatives.
While the improvements are modest, the article demonstrate a potentially advantageous way of modelling the inter-study variability by a hierarchical random effects model.
However, the virtue of such a model is not from improvement in accuracy alone.
Also desirable is the explicit and interpretable quantification of the inter-study variance.
If $\hat{\nu}$ is estimated to be large, the studies exhibit a largely common covariance structure, and vice-versa when $\hat{\nu}$ is small.
We have provided the ICC for the RCM as an attempt to aid in the interpretation of $\nu$.
Also provided is the basic framework for testing if study heterogeneity is present.
However, the proposed testing is computationally heavy and only feasible when $p$ is sufficiently small.
This could e.g.\ be overcome by an improved and faster fitting procedures or by deriving the distribution of $\hat{\nu}$ under the null hypothesis.
Yet the latter is seemingly intractable as $\hat{\nu}$ is a very complex function of the data.
The fact that the null-hypothesis lies on the edge of parameter space also seem to constrain the feasibility of deriving such a distribution.

As demonstrated, combining multiple studies can yield a sufficiently large total sample size $n_\bullet$ that allows for estimation of large covariance matrices without the use of regularization.
The generalization of the model to $p \gg n_\bullet$ is extremely interesting though out of scope for this article.
We believe this work could be further enriched by combining the method with regularized estimation.

The recent advances in such regularized techniques which allows for analysis of very large covariance matrices has unfortunately diminished the focus on collecting larger sample.
Just because it is technically possible, it does not mean a good estimate is achieved.
For example, while non-zero entries often can be accurately recalled in graphical LASSO, actual estimates of the covariances (or precisions) can still be heavily biased.
Large sample-sizes are still needed to achieve unbiased estimates of the covariance due to the bias-variance trade-off.
Therefore, an increased focus should also be appointed to efficiently aggregating datasets and achieving sufficiently large sample sizes to allow for stable and unbiased estimation of covariance matrices.

One might question whether the added utility of the $\nu$ parameter is an obvious relaxation of covariance homogeneity.
For example, it is unclear how large a proportion a \textit{single} extra parameter can explain of the inter-study variance.
Hence, the present work should be considered a first step in the direction of explicitly modeling the inter-study variation.

As an addition to the discriminant analysis toolbox, we recognize that further and more sophisticated simulation experiments are needed to explore scenarios where HDA should be considered a serious alternative.

\section*{Acknowledgments}
Special thanks to Martin Raussen and Jon Johnsen for their help with the mathematical proofs.
We also thank Karen Dyk\ae{}r for her assistance on the biological interpretation of the results.
The helpful statistical comments from Steffen Falgreen were much appreciated. As were the help on the microarray preprocessing workflow from Andreas Petri. The technical assistance from Alexander Schmitz, Julie S.\ B\o{}dker, Ann-Maria Jensen, Louise H. Madsen, and Helle H\o{}holt is also greatly appreciated.



\newpage
\phantomsection
\addcontentsline{toc}{section}{References}
\bibliographystyle{plainnat}
\bibliography{references_manual,references_mendelay}



\newpage
\appendix






\section[Marginalization of Sigma]{Marginalization of $\vSigma$}
\label{sec:marginalization}
This section shows the marginalization over $\vSigma$ in \eqref{eq:loglik}.
For ease of notation we drop the subscript $i$ on $\vSigma_i$, $\vX_i$, $\vS_i = \vX_i \vX_i^\top$, and $n_i$ in the above text.
By the model assumptions,
\small
\begin{align*}
  &f(\vX | \vPsi, \nu) \\
  &= \int f(\vX|\vSigma) f(\vSigma | \vPsi, \nu) d\vSigma \\
  &= \int \left[ \prod_{j = 1}^n  (2\pi)^{-\frac{p}{2}} |\vSigma|^{-\frac{1}{2}}
                         e^{-\frac{1}{2}\tr(\vx_{ij}\vx_{ij}^\top\vSigma^{-1})} \right]
          %\\ &\qquad\times
          \frac{\big|\vPsi\big|^\frac{\nu}{2}}
               {2^{\frac{\nu p}{2}}\Gamma_p\!\left(\frac{\nu}{2}\right)}
          |\vSigma|^{-\frac{\nu+p+1}{2}}e^{-\frac{1}{2}\tr\!\big(\vPsi\vSigma^{-1}\big)}
      \;d\vSigma \\
  &= (2\pi)^{-\frac{np}{2}}
      \frac{\big|\vPsi\big|^\frac{\nu}{2}}
           {2^{\frac{\nu p}{2}}\Gamma_p\!\left(\frac{\nu}{2}\right)}
      \int
        |\vSigma|^{-\frac{n}{2}}  e^{-\frac{1}{2}\tr(\vS\vSigma^{-1})}
        |\vSigma|^{-\frac{\nu+p+1}{2}} e^{-\frac{1}{2}\tr\!\big(\vPsi\vSigma^{-1}\big)}
      \;d\vSigma \\
  &=
      \frac{\big|\vPsi\big|^\frac{\nu}{2}}
           {\pi^\frac{np}{2} 2^{\frac{(\nu + n) p}{2}}\Gamma_p\!\left(\frac{\nu}{2}\right)}
      \int
        |\vSigma|^{-\frac{(\nu + n)+p+1}{2}}
         e^{-\frac{1}{2}\tr\!\Big(\big(\vPsi+ \vS\big)\vSigma^{-1}\Big)}
      \;d\vSigma.
\end{align*}
\normalsize
The integrand can be recognized as a unnormalized inverse Wishart pdf of the distribution $\calW^{-1}\big(\vPsi + \vS, \nu + n\big)$, and so the integral evaluates to the reciprocal value of the normalizing constant in that density. Thus,
\begin{align}
  f(\vX | \vPsi, \nu)
  &=
    \frac{\big|\vPsi\big|^\frac{\nu}{2}}
         {\pi^\frac{np}{2} 2^{\frac{(\nu + n) p}{2} } \Gamma_p\!\left(\frac{\nu}{2}\right)}
    \frac{2^\frac{(v+n)p}{2} \Gamma_p\left(\frac{\nu + n}{2}\right)}
         {\big|\vPsi + \vS\big|^{\frac{\nu + n}{2}}} \notag \\
  &=
    \frac{\big|\vPsi\big|^\frac{\nu}{2} \Gamma_p\left(\frac{\nu + n}{2}\right)}
         {\pi^\frac{np}{2}
           \big|\vPsi + \vS\big|^{\frac{\nu + n}{2}} \Gamma_p\!\left(\frac{\nu}{2}\right)}.
    \label{eq:marg1}
\end{align}
Using the matrix determinant lemma and $\vS = \vX^\top\vX$, this can be further simplified to
\begin{align*}
  f(\vX | \vPsi, \nu)
  &=
  \frac{\big|\vPsi\big|^\frac{\nu}{2} \Gamma_p\left(\frac{\nu + n}{2}\right)}
       {\pi^\frac{np}{2}
         \big|\vI + \vX\vPsi^{-1}\vX^\top\big|^\frac{\nu + n}{2}
         \big|\vPsi\big|^\frac{\nu + n}{2}
         \Gamma_p\!\left(\frac{\nu}{2}\right)} \\
  &=
  \frac{\Gamma_p\left(\frac{\nu + n}{2}\right)}
       {\pi^\frac{np}{2}
          \big| \vI + \vX\vPsi^{-1}\vX^\top\big|^\frac{\nu + n}{2}
          \big|\vPsi\big|^\frac{n}{2}
          \Gamma_p\!\left(\frac{\nu}{2}\right)},
\end{align*}
which can help to speed-up computations.





\section{Proofs}
\label{sec:proofs}
\subsection{Non-concavity of the log-likelihood}
\label{sec:concaveloglik}
The likelihood function is not log-concave in general.
This section analyses the (non)-concavity of the log-likelihood function given in \eqref{eq:loglik}.
More precisely, the following two propositions are proved.

\propositionNonConcavityInPsi*

\propositionConcavityInNu*

\begin{proof}[\textbf{Proof of Proposition \ref{prop:nonconcavityinpsi}}]
Assume $\nu$ is fixed and consider only the terms involving $\vPsi$ in \eqref{eq:loglik}.
We reduce to the one-dimensional case where
\begin{align*}
  \ell(\psi)
  = \frac{k\nu}{2}\log\!\big(\psi\big)
     - \sum_{i = 1}^k \frac{\nu + n_i}{2}\log\!\big(\psi + x_i^2\big)
\end{align*}
which implies
\begin{align*}
  \ell'(\psi)
  &= \frac{k\nu}{2}\frac{1}{\psi}
     - \sum_{i = 1}^k \frac{\nu + n_i}{2}\frac{1}{\psi + x_i^2} \quad \text{ and} \\
  \ell''(\psi)
  &=  - \frac{k\nu}{2}\frac{1}{\psi^2}
      + \sum_{i = 1}^k \frac{\nu + n_i}{2}\frac{1}{\big(\psi + x_i^2\big)^2}.
\end{align*}
We see, that
\begin{align*}
  \lim_{\psi \to 0^+} \ell''(\psi) = - \infty
  \;\text{  and  }\;
  \lim_{\psi \to \infty^-} \ell''(\psi) = 0.
\end{align*}
We draw the likelihood and its derivative with $k = 1$ and other appropriately chosen parameters as seen in Figure \ref{fig:one_dimensional_loglik} which is generated by the \texttt{R}-code:

<<one_dimensional_loglik, echo=-3, fig.height=2, fig.width=6, fig.pos="tb", ig.scap = "", fig.cap = "\\it The one-dimensional log-likelihood and its first and second derivative as a function of $\\psi$.">>=
@

\noindent The log-likelihood $\ell$ is not log-concave as the second derivative is not always negative.
Alternatively, the first derivative is not monotonically non-increasing which is a necessary and sufficient condition for a differentiable function of one variable to be concave.
\end{proof}

\begin{proof}[\textbf{Proof of Proposition \ref{prop:concavityinnu}}]
Consider the terms involving $\nu$.
Clearly, the mixed terms involving both $\nu$ and $\vPsi$ are log-linear in $\nu$ and hence log-concave.
We thus restrict our attention to the remaining terms not dependent on $\vPsi$.
By themselves, the second term in \eqref{eq:loglik},
$
\log \Gamma_p\!\left(\frac{\nu + n_i}{2}\right)
$
is convex since the multivariate gamma function is log-convex, cf.\ section \ref{sec:multigamma}.
In the same manner, the fourth term,
$
  - \log \Gamma_p\!\left(\frac{\nu}{2}\right),
$
is concave by the negative sign.
The sum of these terms, however, are concave in $\nu$, since
\begin{align*}
  &\log\Gamma_p\!\left( \frac{\nu + n_i}{2} \right) -
    \log\Gamma_p\!\left( \frac{\nu}{2} \right)
  =  \log\frac{\Gamma_p\!\left( \frac{\nu + n_i}{2} \right)}{
               \Gamma_p\!\left( \frac{\nu}{2}       \right)}
  = \sum_{j = 1}^p \log
    \frac{\Gamma\!\left( \frac{\nu + 1 - j}{2} + \frac{n_i}{2} \right)}{
          \Gamma\!\left( \frac{\nu + 1 - j}{2} \right)}.
\end{align*}
which can be seen to be concave since $n_i \geq 1$ for all $i$ and
\begin{align}
  h(x) = \log\left(\frac{\Gamma(x + a)}{\Gamma(x)}\right)
  \label{eq:logGammaRatio}
\end{align}
is concave for all $x>0$ and $a > 0$. The concavity of $h$ is easily seen by the fact that
\begin{align*}
  h''(x) = \psi(x + a) - \psi(x) < 0,
\end{align*}
where $\psi(\cdot)$ is the tri-gamma function which is a well-known monotonically decreasing function.
Hence, the log-likelihood is log-concave in $\nu$.
\end{proof}






\subsubsection{log-convexity of the multivariate gamma function}
\label{sec:multigamma}
The log-convexity of the multivariate gamma function can seen using the following characterization of $\Gamma_p$,
\begin{align}
  \label{eq:multigamma}
  \Gamma_p(t) = \pi^{ \frac{1}{2} \binom{p}{2} }
  \prod_{j = 1}^p \Gamma\!\left(t + \frac{1 - j}{2}\right)
  \text{ where }
  \Gamma(t) = \int_0^\infty x^{t-1} e^{-x} dx.
\end{align}
From this
\begin{align}
  \label{eq:logmultigamma}
  \log\Gamma_p(t) =
  \frac{1}{2}\binom{p}{2} \log\pi +
  \sum_{j = 1}^p \Gamma\left(t + \frac{1-j}{2}\right),
\end{align}
which is convex since $\Gamma$ is log-convex and a sum of convex functions is convex.
Hence $\Gamma_p$ is also log-convex.




\subsection{Existence and uniqueness of likelihood maxima}
\label{sec:negativedefinite}
This section proves the following lemmas which imply the proposition in the headline.

\lemmaOne*

\lemmaTwo*

\propositionUniqueMax*

\noindent The log-likelihood in \eqref{eq:loglik}, assuming $\nu$ fixed, obey
\begin{align}
  \label{eq:loglik2}
  2\ell(\vPsi)
  &= c + k\nu\log\big|\vPsi\big| - \sum_{a=1}^k (n_a + \nu)\log\big|\vPsi + \vS_a\big|
\end{align}
Notice that this equation also holds in the reparameterization. Here we have
\begin{align*}
  2\ell(\vSigma)
  &= c + k\nu\log\big|(\nu-p-1)\vSigma\big| - \sum_{a=1}^k (n_a + \nu)\log\big|(\nu-p-1)\vSigma + \vS_a\big|\\
  &= c' + k\nu\log\big|\vSigma\big| - \sum_{a=1}^k (n_a + \nu)\log\big|\vSigma + (\nu-p-1)^{-1}\vS_a\big|.
\end{align*}
Since $(\nu-p-1)^{-1}\vS_a$ is only dependent on data (when $\nu$ is fixed) we can set $(\nu-p-1)^{-1}\vS_a := \vS_a$. Hence, without loss of generality we consider \eqref{eq:loglik2}.

\begin{proof}[\textbf{Proof of Proposition \ref{prop:uniquemax}}]
The proposition follows from Lemma \ref{lem:elltominusinfty} and
Lemma \ref{lem:negativesdefinite}.
We first prove existence of the maximum.
By Lemma \ref{lem:elltominusinfty} and the continuity of $\ell$, the set
\begin{align*}
  \Big\{ \; \vPsi \;\Big|\; \ell(\vPsi) \geq \ell(\vPsi^*) \; \Big\}
\end{align*}
is bounded and closed and hence compact for any $\vPsi^*\succ 0$..
The existence of a maximum follows from the extreme value theorem again by continuity of $\ell$.
A stationary point exists due to Rolle's theorem and the differentiability of $\ell$.

Next, we show the uniqueness of maxima.
Suppose there exists two (or more) countable many stationary points $m_1, m_2, ..., m_N$.
By Lemma \ref{lem:elltominusinfty}, the $\ell(\vPsi)$ has a finite upper bound given by the value of the log-likelihood in those points.
All gradient curves (that is, solutions to $\dot{\vPsi} = \nabla \ell(\vPsi)$) must then converge toward exactly one of the stationary points where $\ell$ monotonically increase along each curve.
The basin of attraction $A_i$ associated to each stationary point $m_i$,
\begin{align*}
   A_i = \Big\{ \quad
     \vPsi^* \in \calS_+ \quad \Big| \quad
     \vPsi(0) = \vPsi^*, \quad
     \lim_{t \to \infty} \vPsi(t) = m_i\quad
  \Big\},
\end{align*}
are open sets in set of all positive semi-definite matrices $\calS_+$.
This partitions the space $\calS_+$ into $N$ open sets.
However, this is only possible if $A_i = A_j$ for all $i$ and $j$ and thus there is only a single basin of attraction and maximum of $\ell$.
\end{proof}




\begin{proof}[\textbf{Proof of Lemma \ref{lem:elltominusinfty}}]
Assume the hypothesis of the lemma and consider the expression given in \eqref{eq:loglik2}.
If $\lambda_i \to \infty$ then
\begin{align*}
  \ell(\vPsi)
  &= \frac{k\nu}{2}\log\big|\vPsi\big| - \sum_{i = 1}^k \frac{\nu + n_i}{2} \log |\vPsi+\vS_i| \\
  &\leq \frac{k\nu}{2}\log|\vPsi| - \sum_{i = 1}^k \frac{\nu + n_i}{2} \log |\vPsi|
  =  - \frac{n_\bullet}{2} \log |\vPsi| \to -\infty.
\end{align*}
This proves the first case where the largest eigenvalue diverge to infinity.
Suppose $\lambda_i \to 0$ and let
\begin{align*}
  C = \sum_{i = 1}^k \frac{(\nu + n_i)}{2} = \frac{k\nu}{2} + \frac{n_\bullet}{2},
\end{align*}
then \eqref{eq:loglik2} can be expressed as
\begin{align*}
  \ell(\vPsi)
  = \frac{k\nu}{2}\log|\vPsi| -
    C \sum_{i = 1}^k \frac{(\nu + n_i)}{2C} \log |\vPsi+\vS_i|.
\end{align*}
Since $\log|\cdot|$ is concave and the above sum is a convex combination, we have
\begin{align*}
  \ell(\vPsi)
  \leq \frac{k\nu}{2}\log|\vPsi| -
     C \log\left| \vPsi + \sum_{i = 1}^k \frac{(\nu + n_i)}{2C}\vS_i\right|.
\end{align*}
Clearly, the first term goes to $-\infty$ whenever an eigenvalue $\lambda_i \to 0$.
The matrix in the second term is almost surely positive definite when $n_\bullet = \sum_{a=1}^k x_a \geq p$ and the log determinant will converge to some constant.
Hence, if $\lambda_i \to 0$ then
\begin{align*}
  \ell(\vPsi) \leq \frac{k\nu}{2}\log|\vPsi| + C_2 \to -\infty,
\end{align*}
which completes the proof.
\end{proof}

\begin{proof}[\textbf{Proof of Lemma \ref{lem:negativesdefinite}}]
The matrix cookbook by \citet{Petersen2008} is a useful reference here. See equations (41, p.\ 8) and (59, p.\ 9) and pages 14 and 52-53 in \citep{Petersen2008}. We first compute expressions for the first and second order derivatives.

\textbf{First order derivatives.}
From the log-likelihood expression, we compute the first order derivative $\nabla_\vPsi 2\ell(\vPsi)$ which is the matrix-valued function where each entry is given by
\begin{align}
  %\left(\frac{\partial 2\ell}{\partial \vPsi}\right)_{ij}=
  \frac{\partial 2\ell}{\partial \Psi_{ij}}
  = k\nu\tr\!\left(\vE^{ij}\vPsi^{-1}\right)
    - \sum_{a = 1}^k (\nu + n_a)\tr\!\left(\vE^{ij}\left(\vPsi + \vS_a\right)^{-1}\right).
\label{eq:dloglik}
\end{align}
where $\vE^{ij}$ is a matrix with ones at entries $(i,j)$ and $(j,i)$ and zeros elsewhere.
This $\vE^{ij}$ is introduced as the derivative is not straight-forward because of the symmetric structure of $\vPsi$. Had $\vPsi$ been unstructured, then $\frac{\partial}{\partial \vPsi}\log|\vPsi| = \vPsi^{-1}$.
However, when $\vPsi$ is symmetric we have that $\frac{\partial}{\partial \Psi_{ij}}\log|\vPsi| = \tr(\vE^{ij}\vPsi^{-1})$ which is to say $\frac{\partial}{\partial \vPsi}\log|\vPsi| = 2\vPsi^{-1} -\vPsi^{-1} \circ \vI$ where $\circ$ denotes the Hadamard product \citep[eq.\ (43) and (141)]{Petersen2008}.

The first order derivative lives in a $\binom{p+1}{2}$-dimensional vector space with basis vectors $\vE^{ij}$ indexed by $(i,j)$, $i\leq j$.


\textbf{Second order derivatives.}
We proceed with the second order derivative $\nabla^2_\vPsi 2\ell(\vPsi)$ with entries given by
\begin{align*}
  \frac{\partial^2 2\ell}{\partial \Psi_{kl} \partial \Psi_{ij}}
  &= - k\nu\tr\!\left( \vE^{ij}\vPsi^{-1} \vE^{kl}\vPsi^{-1} \right) \\
  & + \sum_{a = 1}^k (\nu + n_a)
    \tr\!\left(
      \vE^{ij}\left(\vPsi + \vS_a\right)^{-1}
      \vE^{kl}\left(\vPsi + \vS_a\right)^{-1}
    \right),
\end{align*}
obtained by differentiation of \eqref{eq:dloglik} combined with
$\frac{\partial}{\partial \Psi_{ij}} \vPsi^{-1} = - \vPsi^{-1}\vE^{ij}\vPsi^{-1}$ \citep[eq.\ (40)]{Petersen2008} and the linearity of the trace operator.

The second order derivative can be thought of as a $\binom{p+1}{2} \times \binom{p+1}{2}$-dimensional matrix indexed by $(i,j)$ and $(k,l)$, $i \leq j$, $k \leq l$.

\textbf{Negative definiteness of stationary points.}
With the above expressions we now show that the hessian is negative definite in all stationary points (or, extrema).
Let $\vY = \sum_{(i,j)} y_{ij}E_{ij}$ be a symmetric matrix in the vector space where $\vY \neq \vec{0}$.
The analog to $\vec{y}^\top \vec{A}\vec{y} = y_i \sum_{ij} A_{ij} y_j < 0$ in our vector space then becomes
\begin{align*}
  \sum_{i\leq j, k\leq j}
    Y_{ij}
    \left(\nabla^2_\vPsi 2\ell(\vPsi)\right)_{(i,j),(k,l)}
    Y_{kl}
    < 0
\end{align*}
which amounts to showing that
\small
\begin{align}
- k\nu\tr\!\left( \vY\vPsi^{-1} \vY\vPsi^{-1} \right)
+ \sum_{a = 1}^k (\nu + n_a)
    \tr\!\left(
      \vY\left(\vPsi + \vS_a\right)^{-1}
      \vY\left(\vPsi + \vS_a\right)^{-1}
    \right)
    < 0.
  \label{eq:negativedefinte}
\end{align}
\normalsize
Now, by the positive-definiteness of $\vPsi$, let
\begin{align*}
  \vY &:= \vPsi^{-\frac{1}{2}} \vY \vPsi^{-\frac{1}{2}} \text{ and } \\
  \vS_a &:= \vPsi^{-\frac{1}{2}} \vS_a  \vPsi^{-\frac{1}{2}},
\end{align*}
and we can without loss of generality assume that $\vPsi = \vI$. Hence, the likelihood equation \eqref{eq:loglik2} equated to zero, becomes
\begin{align*}
  k\nu\vI = \sum_a(n_a + \nu)(\vI + \vS_a)^{-1}
\end{align*}
which implies (by multiplication by $\vY$ on each side)
\begin{align}
  k\nu\tr(\vY^2)
  %&= \sum_a (n_a + \nu)\tr\!\left(\vY^2(\vI + \vS_a)\right) \notag\\
  &= \sum_a (n_a + \nu)\tr\!\Big(\vY(\vI + \vS_a)^{-1}\vY\Big).
  \label{eq:loglikequation}
\end{align}
We substitute \eqref{eq:loglikequation} into \eqref{eq:negativedefinte} to get
\begin{align*}
  &\sum_a (n_a + \nu)
  \tr\!\Big(\vY (\vI + \vS_a)^{-1} \vY (\vI + \vS_a)^{-1} - \vY (\vI + \vS_a)^{-1} \vY \Big) \\
  &=  \sum_a (n_a + \nu)
  \tr\!\Big( \vY (\vI + \vS_a)^{-1} \vY \big[ (\vI + \vS_a)^{-1}  - \vI \big]\Big)
  < 0
\end{align*}
We note that $\vS_a = \vX_a\vX_a^\top$ and
\begin{align*}
  (\vI + \vS_a)^{-1} - \vI = -\vX_a\big(\vI + \vX_a^\top\vX_a\big)^{-1}\vX_a^\top,
\end{align*}
by the matrix inversion lemma (or, Woodbury matrix identity) whereby we need to show that
\begin{align*}
  \sum_a (n_a + \nu) \tr\!\Big(
    \vY(\vI + \vX_a\vX_a^\top)^{-1}\vY\vX_a\big(\vI + \vX_a^\top\vX_a\big)^{-1}\vX_a^\top
  \Big)
  > 0.
\end{align*}
Since $(\vI + \vX_a\vX_a^\top)^{-1}\succ 0$ we obtain that
\begin{align*}
  \vY\vX_a(\vI + \vX_a\vX_a^\top)^{-1}\vX_a^\top \vY = 0
  \quad\text{ for } a = 1, ...., k.
\end{align*}
Again by $(\vI + \vX_a\vX_a^\top)^{-1}\succ 0$ we conclude that $\vY\vX_a = 0$ for all $a = 1,...,k$, i.e.\
$\vY(\vX_1, ..., \vX_k) = 0$. If $n_\bullet\geq p$ then almost surely $(\vX_1, ..., \vX_k)$ has rank $p$ whereby $\vY=0$.

% Since $n_a + \nu \geq 1$ and $\vI + \vX_a\vX_a^\top \succeq \vI$, it is enough to shown
% \begin{align*}
%   \sum_a \tr\!\Big(
%     \vY\vY\vX_a\big(\vI + \vX_a^\top\vX_a\big)^{-1}\vX_a^\top
%   \Big)
%   > 0.
% \end{align*}
% Let $\lambda_a$ be the maximum eigenvalue of
% $\vX_a^\top\vX_a$
% and let
% $\lambda_M = \max_a \lambda_a$.
% Then
% $\big(\vI + \vX_a\vX_a^\top\big)^{-1} \succeq (1 + \lambda_M)^{-1}\vI$
% and we need to show
% \begin{align}
%   \label{eq:lasteq}
%   (1 + \lambda_M)^{-1} \sum_a \tr\!\Big( \vY\vY\vX_a\vX_a^\top \Big)
%   \geq
%   \tr\!\left( \vY\vY \sum_a \vX_a\vX_a^\top \right)
%   > 0
% \end{align}
% However, as we assume that
% $\sum_a \vX_a\vX_a^\top \succ 0$,
% then \eqref{eq:lasteq} holds and the proof is complete.

% Since $\vS_a$ is positive semi-definite, it can be diagonalized $\vS_a = \vU_a \vD_a \vU_a^\top$ by the spectral theorem where the diagonal matrix $\vD_a \succeq 0$ (all non-zero entries are positive) and $\vU_a$ is orthonormal (i.e.\ $\vU_a\vU_a^\top = \vU_a^\top\vU_a = \vI$). Using the diagonalization,
% \begin{align}
%   \sum_a (n_a + \nu)\tr\!\Big(
%     \underbrace{\vU^\top \vY (\vI + \vS_a)^{-1} \vY \vU}_{\succeq 0}
%     \big[ (\vI + \vD_a)^{-1}  - \vI \big]
%   \Big)
%   \leq 0
%   \label{eq:loglikequation2}
% \end{align}
% where the underbraced matrix is positive semi-definite and hence have non-negative diagonal elements.
% Furthermore, since $\vD_a$ is diagonal also with non-negative elements, the diagonal matrix $(\vI + \vD_a)^{-1}  - \vI$ clearly have non-positive entries and is thus negative semi-definite.
%
% Since the trace of a matrix product is the sum of the element-wise products, the trace (and thus the sum) will always be non-positive and hence \eqref{eq:loglikequation2} will always hold.
\end{proof}



\section{Likelihood of the precision matrix}
\label{sec:precisionloglik}
Suppose we have $k$ i.i.d. realizations, $\vDelta_1, ..., \vDelta_k$, from the Wishart distribution given in equation \eqref{eq:precisiondensity}. The corresponding log-likelihood can be computed straight-forwardly:
\begin{align*}
  \ell(\vTheta | \vDelta_1, ..., \vDelta_k)
  &= \sum_{i = 1}^k \log f(\vDelta_i | \vTheta) \\
  &= \sum_{i = 1}^k \log
    \frac{\big|\vTheta\big|^{-\frac{\nu}{2}}}
         {2^{-\frac{vp}{2}}\Gamma_p\!\left(\frac{\nu}{2}\right)}
    |\vDelta_i|^\frac{\nu - p - 1}{2}e^{-\frac{1}{2}\tr\!\big(\vTheta^{-1}\vDelta_i\big)}\\
   &= c + \sum_{i = 1}^k \left(
     -\frac{\nu}{2} \log \big|\vTheta\big|
     -\frac{1}{2}\tr\!\big(\vTheta^{-1}\vDelta_i\big)
     \right)\\
   &= c - \frac{\nu k}{2}
     \left(
       \log |\vTheta| +
       \tr\!\left(\vTheta^{-1} \frac{1}{\nu k}\sum_{i = 1}^k\vDelta_i\right)
     \right).
\end{align*}
The last expression is to be maximized with respect to $\vTheta$ and can be recognized as the MLE problem in a multivariate Gaussian distribution. Hence,
\begin{align*}
  \vTheta = \frac{1}{k \nu} \sum_{i = 1}^k \vDelta_i,
\end{align*}
is the MLE in this model.




\section{Approximate maximum likelihood estimator}
\label{sec:amle}
To find the maximizing parameters we differentiate \eqref{eq:loglik} w.r.t.\ $\vPsi$ and equate to zero while assuming $\nu$ known and constant.
The first order derivative can be seen in equation \eqref{eq:dloglik}. Equating zero we have that
\begin{align}
  \vec{0}
  &= \frac{k\nu}{2} \vPsi^{-1}
    - \sum_{i=1}^k \frac{\nu + n_i}{2}
      (\vPsi + \vec{S}_i')^{-1}
  \label{eq:firstordderivloglik} \\
  &= \frac{k\nu}{2} \vPsi^{-1}
    - \sum_{i=1}^k \frac{\nu + n_i}{2}
      \left(\vec{I} + \vPsi^{-1}\vS_i\right)^{-1}\vPsi^{-1}.
      \notag
\end{align}
This implies
\begin{align}
  k\nu \vec{I}
    - \sum_{i=1}^k (\nu + n_i)
      \left(\vec{I} - (-\vPsi^{-1}\vS_i)\right)^{-1}
   = \vec{0}.
\end{align}
which can be rewritten as
\begin{align*}
    k\nu \vec{I}
    - \sum_{i=1}^k      (\nu + n_i)
      \sum_{l=0}^\infty \left(-\vPsi^{-1}\vS_i\right)^{l}
   = \vec{0}.
\end{align*}
by the Neumann series $\left((\vI + \vec{A})^{-1} = \sum_{l = 0}^\infty \vec{A}^l\right)$ if $\lim_{l \to \infty} (I - \vPsi^{-1}\vS_i)^l = \vec{0}$ for all $i$, i.e.\ the eigenvalues of $\vPsi^{-1}\vS_i$ should be less than $1$.

Thus, we can approximate the solution by using the first order expansion $(l = 1)$ and solve for $\vPsi$
\begin{align*}
  \vec{0}
  &= k\nu\vec{I} - \sum_{i=1}^k (\nu + n_i)(\vec{I} - \vPsi^{-1}\vS_i) \\
  &= k\nu\vec{I}
   - k\nu\vec{I}
   - n_\bullet\vec{I}
   + \vPsi^{-1}\sum_{i=1}^k \nu\vS_i
   + \vPsi^{-1}\sum_{i=1}^k n_i\vS_i \\
  &= - n_\bullet\vec{I}
     + \vPsi^{-1}\sum_{i=1}^k (\nu + n_i) \vS_i
\end{align*}
where $n_\bullet = \sum_{i=1}^k n_i$ is the total number of observations. This implies
\begin{align*}
   \vPsi^{-1}\sum_{i=1}^k (\nu + n_i) \vS_i
    = n_\bullet \vec{I}
\end{align*}
which suggest the estimator
\begin{align}
  \hat{\vPsi}_\text{MLE}
  = \frac{\sum_{i=1}^k (\nu + n_i) \vec{S}_i}{n_\bullet},
  \label{eq:mle}
\end{align}
and
\begin{align}
  \hat{\vSigma}_\text{MLE}
  = \frac{\sum_{i=1}^k (\nu + n_i) \vec{S}_i}{(\nu-p-1)n_\bullet}.
\end{align}
These estimate are seen to correspond to a weighted sum of the scatter matrices.



\section{Module member genes}
\label{sec:module_member_genes}
A printout of the genes in each module

<<print_dlbcl_module_genes>>=
print(lapply(dlbcl.module.genes, unname))
@

\section{Significant GO terms}
\label{sec:sig_go_term}
Tables of the significant GO terms.

<<GO_tabs, echo=FALSE, results='asis'>>=
@



\end{document}

