\documentclass{article}
\input{preamble.tex}

\title{Graphical meta-analysis: Estimation of covariance matrices from multiple studies}
\author{
  Anders Ellern Bilgrau \\ \texttt{abilgrau@math.aau.dk} \and
  Poul Svante Eriksen \\ \texttt{svante@math.aau.dk} \and
  Martin B\o{}gsted \\ \texttt{m\textunderscore{}boegsted@dcm.aau.dk}
}

\begin{document}

<<initalize_knitr, echo=FALSE, results='hide', message=FALSE>>=
library("knitr")
options(width = 80)
opts_chunk$set(size = "footnotesize", fig.align = "center")
read_chunk("RCM.R")
@
<<initialize_script, echo=FALSE, results='hide', message=FALSE>>=
@
<<auxiliary_functions, echo=FALSE>>=
@

\listoffixmes

\maketitle
\begin{abstract}
We propose a hierarchical graphical model and estimators for a common covariance matrix in cases where multiple groups or datasets are present.
It is applicable where the groups or datasets are believed to share a common covariance of interest obscured by noise.
As such, it provides a basis for meta-analysis of gaussian graphical models where the groups are formed by datasets.
Our approach is inspired by traditional meta-analysis using random effects models.
We derive the basic properties and estimators of the model and compare our estimators to the straight-forward approaches of simple averages (which can also be derived from the model) or ``mega analysis'' were the datasets are combined, reprocessed, and plugged into the usual estimators.
Though only a modest improvement, explicitly accounting for the inter-study variance is superior to the alternative.
The model is also shown to be applicable as an intermediate between linear discriminant analysis (LDA) and quadratic discriminant analysis (QDA). \medskip

\noindent \textbf{Keywords:} \textit{meta-analysis, covariance estimation, integrative analysis, network integration, structural meta-analysis, gaussian graphical modeling, linear discriminant analysis, quadratic discriminant analysis}
\end{abstract}

\newpage
\tableofcontents

\newpage
\section{Introduction}
The fundamental problem in statistics of accurately and precisely estimating the covariance matrix (or its inverse) is notoriously difficult though computationally easy.
The usual bias-corrected maximum likelihood estimator (MLE), the sample covariance matrix, has long been known to perform poorly in general due to high variability \citep{Dempster1972}.
The sample covariance become increasingly ill-conditioned as number of variables $p$ approaches the sample size $n$ and singular $p$ exceeds $n$.
Because of its central statistical role the list of statistical methods and applications utilizing the estimated covariance matrix is long.
Beside the many standard statistical methods such as principal component analysis (PCA), linear discriminant analysis (LDA), and quadratic discriminant analysis (QDA), more direct applications include [examples in medicine, biology, genetics, finance, forensics, physics, engineering, signal processing].
Among this expanding list of applications is also an increasing number of high-dimensional applications and datasets publicly available at online repositories.

In high-dimensional datasets the number of features $p$ often far exceed the number of samples $n$.
Since the number of parameters increase quadratically in $p$ and the sample covariance become singular when $p > n$ a plethora of shrinkage and regularization estimators have been proposed to combat the accompanying problems by effectively increasing the degrees of freedom.
In cases where $p \approx n$ the covariance estimate become ill-conditioned and its inversion numerically unstable.
Instead of attempting to derive still more sophisticated estimators with diminishing improvements we attempt to alleviate the problem from a different angle by using more available data and thus effectively increasing $n$.
While the extension to $p > n$ is very important it is out of scope and we restrict ourselves to $p < n$ for the purposes of this paper.

This work was motivated by gene-gene interaction networks in diffuse large B-cell lymphoma (DLBCL) where the covariance matrix contain all information about the conditional dependencies of the genes. However, the methods are general and not limited to such genomic data.
As with many other cancers, a large number of DLBCL studies are now available online and hence we wish to use these studies in combination with our own data to arrive at a good estimate of the covariance matrix and the inter-study variation.




\section{A graphical random effects model}
Meta-analysis comes principally in two different flavors, fixed and random, corresponding to the assumption on the nature of the treatment effect.
Random-effects models (REM) for meta-analysis model the effects as random variables realized for each study.
In a vein similar to the regular meta-analysis by \citet{Choi2003}, we think of the the different studies as related but perturbed experiments.
In graphical modeling, the true covariance matrix is the effect of interest to be estimated.
Hence, a graphical analog to \citet{Choi2003} is the following relatively simple random covariance model (RCM) of the observations.
Let $p$ be the number of features and $k$ the number of studies.
We model each sample of the $i$th study as a $p$-dimensional zero-mean multivariate gaussian vector with covariance matrix realized from an inverse Wishart distribution, i.e.\ the hierarchical model
\begin{align}
\begin{split} \label{eq:REM}
  \vSigma_i | \vPsi, \nu  &\sim \calW^{-1}(\vPsi, \nu), \\
  \vx | \vSigma_i &\sim \calN_p(\vec{0}_p, \vSigma_i), \qquad i = 1, ..., k,
\end{split}
\end{align}
where $\calW^{-1}(\vPsi, \nu)$ denotes an inverse Wishart distribution with probability density function (pdf),
\begin{align*}
  f(\vSigma_i | \vPsi, \nu) =
  \frac{ |\vPsi|^\frac{\nu}{2} }{
        2^\frac{\nu p}{2} \Gamma_p\!\left( \frac{\nu}{2} \right) }
        |\vSigma_i|^{-\frac{\nu+p+1}{2}}
  e^{-\frac{1}{2} \tr(\vPsi\vSigma_i^{-1})},
\end{align*}
$\vPsi$ and $\vSigma_i$ are positive definite, $\nu > p - 1$, and $\calN_p(\vec{0}_p, \vSigma_i)$ denotes a multivariate gaussian distribution with pdf
\begin{align*}
  f(\vec{x} | \vSigma_i) =
  (2\pi)^{-\frac{p}{2}} \det(\vSigma_i)^{-\frac{1}{2}}
  e^{ -\frac{1}{2}\vx^\top \vSigma_i^{-1}\vx }.
\end{align*}
Throughout this paper, we use the generic notation $f(\cdot | \cdot)$ and $f(\cdot)$ for the conditional and unconditional pdf of random variables, respectively.
In the model, $\nu$ controls the inter-study variation where the $(\nu - p - 1)\vSigma_i$ concentrate around $\vPsi$ for $\nu\to\infty$. I.e.\ the inter-study variation goes to zero for larger $\nu$. We wish to infer the parameters $\vPsi$ and $\nu$ from the observed data so the common expected covariance matrix
\begin{align}
  \label{eq:expcovar}
  \vSigma = \bbE[\vSigma_i] = \frac{1}{\nu - p - 1} \vPsi
\end{align}
can be assessed.




\subsection{The likelihood function}
Suppose we have $n_i$ i.i.d.\ observations, $\vx_{i1}, \dots,\vx_{in_i}$, from the $i = 1,...,k$ independent studies from the model given in \eqref{eq:REM}.
Let $\vX_i = (\vx_{i1}, \dots,\vx_{in_i})^\top$ be the $n_i \times p$ of the $i$'th study where rows correspond to samples and columns to variables.
For the archetypal example in genomics, $\vX_i$ is transposed gene expression matrix with genes in the columns and samples in the rows.
By the independence assumptions, the log-likelihood for $\vPsi$ and $\nu$ is given by
\begin{align*}
  &\ell\!\left(\vPsi, \nu \big|\vX_1, ..., \vX_k  \right)
  = \log f\!\left(\vX_1, ..., \vX_k \big| \vPsi, \nu \right) \\
  &= \log\!\int %\cdots \int
               f(\vX_1, ...,\vX_k |
               \vSigma_1, ..., \vSigma_k, \vPsi, \nu)
             f(\vSigma_1, ..., \vSigma_k | \vPsi, \nu)
             d\vSigma_1 \cdots d\vSigma_k \\
  &= \log\!\int %\cdots \int
               \prod_{i=1}^k
               f(\vX_i | \vSigma_i)
               f(\vSigma_i | \vPsi, \nu)
               d\vSigma_1 \cdots d\vSigma_k \\
  &= \sum_{i=1}^k \log\!\int
               f(\vX_i | \vSigma_i)
               f(\vSigma_i | \vPsi, \nu)
               d\vSigma_i.
\end{align*}
Since the inverse Wishart distribution is conjugate to the multivariate gaussian distribution the integral, of which the integrand forms a gaussian-inverse-Wishart distribution, can be evaluated. Hence $\vSigma_i$ can be marginalized out, cf.\  Appendix \ref{sec:marginalization}, and we arrive at the following expression for the log-likelihood function,
\small
\begin{align}
  &\ell\!\left(\vPsi, \nu \big| \vX_1, ..., \vX_k \right) %\notag\\
  = \sum_{i=1}^k \log
    \frac{|\vPsi|^\frac{\nu}{2} \Gamma_p\!\left(\frac{\nu+n_i}{2}\right)}
         {2^\frac{n_i p}{2} |\vPsi +\vX_i^\top\vX_i|^\frac{\nu+n_i}{2}
          \Gamma_p\!\left(\frac{\nu}{2}\right)}          \notag\\
  &= c + \sum_{i=1}^k \!\bigg[
            \frac{\nu}{2}  \log |\vPsi|
            + \log \Gamma_p\!\left(\frac{\nu + n_i}{2}\right)
            - \frac{\nu + n_i}{2}\log|\vPsi +\vX_i^\top\vX_i|
            - \log \Gamma_p\!\left(\frac{\nu}{2}\right)
            \!\bigg]\!,
    \label{eq:loglik}
\end{align}
\normalsize
where the constant terms are abbreviated by $c$ and $\Gamma_p$ is the multivariate generalization of the gamma function $\Gamma$ seen in Appendix \ref{sec:multigamma}.
As should be expected, the scatter matrix $\vec{S}_i =\vX_i^\top\vX_i$ and study sample size $n_i$ are sufficient statistics for each study.
Note also that $\vS_i$ is Wishart distributed $\vS_i|\vSigma_i ~ \calW(\vPsi, n_i)$ by construction.

In general, the likelihood is not log-concave, cf.\ Proposition \ref{prop:nonconcavityinpsi} in Appendix \ref{sec:concaveloglik}.
We show that it is log-concave in $\nu$ for fixed $\vPsi$, cf.\ Proposition \ref{prop:concavityinnu} in Appendix \ref{sec:concaveloglik}, and hence there exists a unique maxima for the marginal $\ell(\nu)$.
Reversely, for fixed $\nu$ we show that the Hessian in any stationary point (i.e.\ where $\frac{\partial\ell}{\partial\vPsi} = 0$) is negative semi-definite, cf.\ Lemma \ref{lem:negativesemidefinite} in Appendix \ref{sec:negativedefinite}, and hence a local maxima.
This combined with the observation that $\ell(\vPsi) \to -\infty$ whenever an eigenvalue $\lambda_i \to 0$ or $\lambda_i \to \infty$, cf.\ Lemma \ref{lem:elltominusinfty} in Appendix \ref{sec:negativedefinite}, we prove that any stationary point indeed is a global maximum, cf.\ Proposition \ref{prop:uniquemax} in Appendix \ref{sec:negativedefinite}.
Hence we have existence and uniqueness of a global maximum in $\vPsi$.

\subsection{Moment estimate}
In the following, estimation of $\vPsi$ for fixed $\nu$ is considered.
With the RCM framework, we show that, properly scaled, the pooled covariance and mean covariance matrices can be viewed as different moment estimators.
These straight-forward estimators are thus movtivated by the model.
Simple estimates of $\vPsi$ are readily available by the hierachical construction.

By the assumptions the first and second moment of the $j$th observation in the $i$th study, $\vx_{ij}$, is given by
\begin{align*}
  \bbE[\vx_{ij}] &= \vec{0} \\
  \bbE[\vx_{ij}\vx_{ij}^\top]
    &= \bbE\!\left[\bbE[\vx_{ij}\vx_{ij}^\top | \vSigma_i ]\right]
    = \bbE[\vSigma_i]
    = \frac{\vPsi}{\nu - p - 1}.
\end{align*}
for all $j = 1, ..., n_i$ and $i = 1, ..., k$. This yield the estimator
\begin{align}
  \label{eq:pooledest}
  \hvPsi_\text{pool}
  = (\nu - p - 1)\frac{\sum_{i = 1}^k \vS_i}{\sum_{i = 1}^k n_i},
\end{align}
which is a scaled version of the pooled covariance matrix.

Alternatively and in the same manner, the expectation of a single scatter matrix is
\begin{align*}
  &\bbE[ \vec{S}_i ]
    = \bbE[\vX_i^\top\vX_i ]
    = \bbE\!\left[\bbE[\vX_i^\top\vX_i | \vSigma_i ]\right]
    = \bbE[n_i \vSigma_i]
    = n_i\bbE[\vSigma_i]
    = n_i\frac{\vPsi}{\nu - p - 1},
\end{align*}
which gives the estimator
\begin{align}
  \label{eq:meanestimate}
  \hat{\vPsi}_\text{mean}
  %= \frac{1}{k} \sum_{i = 1}^k \frac{\nu - p - 1}{n_i}\vX_i^\top\vX_i
  = (\nu - p - 1) \frac{1}{k} \sum_{i = 1}^k \frac{1}{n_i}\vS_i.
\end{align}
This estimate is a scaled average of the sample covariance matrices.
Both estimates of $\vPsi$ are seen to be unbiased.\fxnote{Can we say anything about efficiency and consistency?}
Note that if the common or expected covariance matrix is of interest, then the pooled and mean covariance are obtained as estimators by plugging \eqref{eq:pooledest} and \eqref{eq:meanestimate} into \eqref{eq:expcovar}, respectively, which cancels out the scaling factor.



\subsection{Maximization of the likelihood}
To find the maximizing parameters we differentiate \eqref{eq:loglik} w.r.t.\ $\vPsi$ and equate to zero while assuming $\nu$ known and constant. Assuming $\vPsi$ unstructured, then $\nabla_{\vec{Z}} \log|\vec{Z}| = \vec{Z}^{-1}$ and
\begin{align}
  \vec{0}
%   =\nabla_\vPsi \ell(\vPsi)
  &= \frac{k\nu}{2} \vPsi^{-1}
    - \sum_{i=1}^k \frac{\nu + n_i}{2}
      (\vPsi + \vec{S}_i)^{-1}
  \label{eq:firstordderivloglik} \\
%   &= \frac{k\nu}{2} \vPsi^{-1}
%     - \sum_{i=1}^k \frac{\nu + n_i}{2}
%       (\vPsi(\vec{I} + \vPsi^{-1}\vec{S}_i))^{-1}
%   \notag \\
  &= \frac{k\nu}{2} \vPsi^{-1}
    - \sum_{i=1}^k \frac{\nu + n_i}{2}
      \left(\vec{I} + \vPsi^{-1}\vec{S}_i\right)^{-1}\vPsi^{-1}.
      \notag
\end{align}
While assuming $\vPsi$ unstructured is not strictly correct it (in this case) leads to a valid equation. The proper 1.\ order derivative can in alternative notation be seen in equation \eqref{eq:dloglik} and can easily be seen to be equivalent to the above.
Equation \eqref{eq:firstordderivloglik} is equivalent to
\begin{align*}
    k\nu \vec{I}
    - \sum_{i=1}^k (\nu + n_i)
      \left(\vec{I} - (-\vPsi^{-1}\vec{S}_i)\right)^{-1}
   = \vec{0}.
\end{align*}
which can be rewritten as
\begin{align*}
    k\nu \vec{I}
    - \sum_{i=1}^k      (\nu + n_i)
      \sum_{l=0}^\infty \left(-\vPsi^{-1}\vec{S}_i\right)^{l}
   = \vec{0}.
\end{align*}
by the Neumann series $\left((\vI + \vec{A})^{-1} = \sum_{l = 0}^\infty \vec{A}^l\right)$ if $\lim_{l \to \infty} (I - \vPsi^{-1}\vec{S}_i)^l = \vec{0}$ for all $i$, i.e.\ the eigenvalues of $\vPsi^{-1}\vec{S}_i$ should be less than $1$.

Thus, we can approximate the solution by using the first order expansion $(l = 1)$ and solve for $\vPsi$
\begin{align*}
  \vec{0}
  &= k\nu\vec{I} - \sum_{i=1}^k (\nu + n_i)(\vec{I} - \vPsi^{-1}\vec{S}_i) \\
  &= k\nu\vec{I}
   - k\nu\vec{I}
   - n_\bullet\vec{I}
   + \vPsi^{-1}\sum_{i=1}^k \nu\vec{S}_i
   + \vPsi^{-1}\sum_{i=1}^k n_i\vec{S}_i \\
  &= - n_\bullet\vec{I}
     + \vPsi^{-1}\sum_{i=1}^k (\nu + n_i) \vec{S}_i
\end{align*}
where $n_\bullet = \sum_{i=1}^k n_i$ is the total number of observations. This implies
\begin{align*}
   \vPsi^{-1}\sum_{i=1}^k (\nu + n_i) \vec{S}_i
    = n_\bullet \vec{I}
\end{align*}
which is equivalent to
\begin{align}
  \hat{\vPsi}_\text{MLE}
  %=  \sum_{i=1}^k \frac{\nu + n_i}{ n_i} \vec{S}_i
  = \frac{\sum_{i=1}^k (\nu + n_i) \vec{S}_i}{n_\bullet}
  = \frac{\sum_{i=1}^k (\nu + n_i)\vX_i^\top\vX_i}{\sum_{i=1}^k n_i},
  \label{eq:mle}
\end{align}
corresponding to a weighted sum of the scatter matrices.




\subsection{Maximization using the EM algorithm}
We now derive the updating scheme of the expectation-maximization (EM) algorithm for fixed $\nu$.
Suppose $\hvPsi_{(t)}$ is the current estimate of $\vPsi$ and that $\vS_i = \vX_i^\top\vX_i = \sum_{j = 1}^{n_i} \vx_{ij} \vx_{ij}^\top$ is the empirical scatter matrix where $\vX_i = (\vx_{i1}, ..., \vx_{in_i})^\top$.
We now compute the expectation step of the EM-algorithm.

From \eqref{eq:REM} we have that,
\begin{align*}
  \vS_i | \vSigma_i  &\sim \calW_p(\vSigma_i, n_i), \qquad i = 1, ..., k\\
  \vSigma_i          &\sim \calW_p^{-1}(\vPsi, \nu)
\end{align*}
Let $\vDelta_i = \vSigma_i^{-1}$ be the precision matrix (or, concentration matrix) and let $\vTheta = \vPsi^{-1}$, then
\begin{align}
  \vS_i | \vDelta_i
  &\sim \calW_p( \vDelta_i^{-1}, n_i)
  &\Leftrightarrow & &
  f(\vS_i | \vDelta_i)
  &\propto
      |\vDelta_i|^{\frac{1}{2}}
       e^{-\frac{1}{2}\tr(\vDelta_i\vS_i)},
  \label{eq:precisiondensity}
  \\
  \vDelta_i
  &\sim \calW_p(\vTheta, \nu)
  &\Leftrightarrow & &
  f(\vDelta_i)
  &\propto
       |\vTheta|^{-\frac{\nu}{2}}
       e^{-\frac{1}{2}\tr(\vTheta^{-1}\vDelta_i)}.
  \notag
\end{align}
From the conjugacy of the inverse Wishart and the Wishart distribution, we have the posterior distribution for the precision matrix,
\begin{align*}
    \vDelta_i | \vS_i
    &\sim \calW_p\!\left( (\vTheta^{-1} + \vS_i)^{-1}, n_i + \nu\right)
\end{align*}
Hence, the expectation step is given by
\begin{align*}
  \bbE[\vDelta_i |\vS_i] = (n_i + \nu)(\vTheta^{-1} + \vS_i)^{-1}.
\end{align*}
The maximization step, in which the log-likelihood $\ell(\vTheta|\vDelta_1, ..., \vDelta_k)$ is maximized, yields the estimate
\begin{align*}
 \hat{\vTheta} = \frac{1}{k\nu}\sum_{i = 1}^k \vDelta_i,
\end{align*}
which is the mean of the scaled precision matrices $\frac{1}{\nu}\vDelta_i$.
The derivation of this estimate can be seen in Appendix \ref{sec:precisionloglik}.
The above yield the updating scheme
\begin{align}
  \label{eq:em}
  \hvTheta_{(t+1)}
  = \frac{1}{k\nu}\sum_{i = 1}^k
    (n_i + \nu)\left(\hvTheta_{(t)}^{-1} + \vS_i\right)^{-1}
\end{align}
for $\vTheta$. The connection to the likelihood equation \eqref{eq:firstordderivloglik} is immediately seen. We denote the estimate obtained by \eqref{eq:em} with $\hvPsi_\text{ML}$.

\subsection{Estimation procedure}
We propose an alternating procedure between estimating $\nu$ and $\vPsi$ while keeping the other fixed.
Given parameters $\hat{\nu}_{(t)}$ and $\hvPsi_{(t)}$ at iteration $t$, we estimate $\hvPsi_{(t+1)}$ using the fixed $\hat{\nu}_{(t)}$. Subsequently, we find $\hat{\nu}_{(t+1)}$ by a standard one-dimensional numerical optimization procedure using the fixed $\hvPsi_{(t+1)}$.
This coordinate ascent-like\fxnote{Or what type of algorithm is it?} approach is repeated until convergence.
More precisely, in pseudo-code, we propose the algorithm seen in Algorithm \ref{alg:RCM}.
\begin{algorithm}
\caption{Pseudo-code for the RCM estimation procedure}\label{alg:RCM}
\begin{algorithmic}[1]
\Procedure{RCM coordinate ascent}{}
\State \algorithmicrequire{
\State \emph{Sufficient data:} $(\vS_1, n_1), ..., (\vS_k, n_k)$
\State \emph{Initial parameters:} $\hvPsi_{(0)}, \hat{\nu}_{(0)}$
\State \emph{Convergence criterion:} $\varepsilon > 0$
}
\State \algorithmicensure{
\State \emph{Parameters:} $\hvPsi_{(t')}, \hat{\nu}_{(t')}$
}
\State
\State \emph{Initialize}: $l_{(0)} \gets \ell(\hvPsi_{(0)}, \hat{\nu}_{(0)})$
\For {$t = 1, 2, 3, ...$}
  \State $\hvPsi_{(t)} \gets U\!\left(\hvPsi_{(t-1)}, \hat{\nu}_{(t-1)}\right)$
  \State $\hat{\nu}_{(t)} \gets \argmax_\nu \ell\!\left(\hvPsi_{(t)}, \nu\right)$
  \State $l_{(t)} \gets \ell\!\left(\hvPsi_{(t)}, \hat{\nu}_{(t)}\right)$
    \If {$l_{(t)} - l_{(t-1)} < \varepsilon$}
      \State \Return $\left(\hvPsi_{(t)}, \nu_{(t)}\right)$
    \EndIf
 \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}
The update function $U$ in the algorithm is defined by the derived estimators above.
That is, equations \eqref{eq:meanestimate}, \eqref{eq:mle}, or \eqref{eq:em} define $U$ to be the moment, approximate MLE, and EM estimate respectively.

This algorithm utilize the results about the log-likelihood and thus provide us with a guarantee of convergence along with the advantage of a very simple implementation.
However the obvious disadvantage is that the identified maxima might be a saddle-point when considering the log-likelihood jointly in $(\vPsi, \nu)$.




\section{Assessment of estimation procedure}
<<numerical_experiment, echo=FALSE, results='hide'>>=
@
To assess the precision and stability of the estimation procedure we generated data from \eqref{eq:REM} for $p = \Sexpr{par.ne[["p"]]}$ variables in $k = \Sexpr{par.ne[["k"]]}$ studies each with an equal number of observations, $n = n_1 = n_2 = n_3$. We chose the parameters $\nu = \Sexpr{par.ne[["nu"]]}$ and
\begin{align*}
\vPsi =
  \begin{bmatrix}
     1 & 0.5 & 0.5 & \cdots\\
     0.5 & 1 & 0.5 & \cdots\\
     0.5 & 0.5 & 1 & \cdots\\
     \vdots & \vdots & \vdots & \ddots
  \end{bmatrix}.
\end{align*}
The number of observations is each study $n$ was varied range in the range $\Sexpr{paste0("[", min(par.ne$n.obs), ",", max(par.ne$n.obs), "]")}$.

We measure the precision of the estimates values against the expected covariance matrix given by \eqref{eq:expcovar}.
Let $\hvPsi$ and $\hat{\nu}$ be the estimates obtained using the moment, EM, or approximate MLE as described.  We then use the plug-in estimate of the expected covariance matrix $\vSigma$ by plugging the estimates into \eqref{eq:expcovar},
\begin{align}
  \label{eq:sigma_plugin}
  \hat{\vSigma} = \frac{1}{\hat{\nu} - p - 1}\hvPsi
\end{align}
of $\vSigma$. We benchmark the proposed estimators. The benchmarking measure used is the following sum of squared errors,
\begin{align*}
  \text{SSE}(\hvSigma) = \sum_{i \leq j} \frac{(\hat{\Sigma}_{ij}- \Sigma_{ij})^2}{\var(\Sigma_{ij})}
\end{align*}
where
\begin{align*}
 \var(\Sigma_{ij}) = n(\Psi_{ij}^2 + \Psi_{ii}\Psi_{jj}).
\end{align*}

For each $n$ from $\Sexpr{min(par.ne$n.obs)}$ to $\Sexpr{max(par.ne$n.obs)}$, the squared sum of errors for each estimator, $\text{SSE}(\hvSigma)$, where computed for $\Sexpr{par.ne[["n.sims"]]}$ datasets and the average of these values seen in Figure \ref{fig:numerical_experiment_plot} as function of the number of samples in each dataset $n_i$.
<<numerical_experiment_plot, echo=FALSE, results='markup', fig.pos="tb", fig.height=7, fig.width=7*2, out.width = ".8\\linewidth", fig.scap = "", fig.cap=paste0("\\it The average sum of squared errors (SSE), of ", par.ne$n.sims, " simulations, as a function of the number of samples $n_i$ in each study.")>>=
@
Unsurprisingly, the proper EM estimation outperform the simple alternative estimates.
We see that the EM estimation is the superior to that of the approximate MLE and moment.
Also seen is that the moment estimate yield the exact same results as using the mean, which could be expected by looking at \eqref{eq:meanestimate}, \eqref{eq:sigma_plugin}, and \eqref{eq:pooledest}.\fxnote{Is this result surprising/trivial? I cannot really tell.}


\subsection{Implementation and availability}
The algorithm along with the different estimators is implemented in the statistical programming language R \cite{R} with workhorse functions in C++ using packages Rcpp and RcppArmadillo \citep{Eddelbuettel2011, RcppArmadillo}.
They are incorporated in the open-source R-package \texttt{correlateR} freely available for forking and editing at \url{http://github.com/AEBilgrau/correlateR}.
See the documentiation here for further details.




\section{Applications}
\subsection{Graphical meta-analysis in DLBCL}
<<dlbcl_analysis, echo=FALSE, results='hide', message=FALSE>>=
@
Diffuse large B-cell lymphoma (DLBCL) is an aggresive cancer subtype of accounting for approximately $31 \%$ \citep{Project1997} of non-Hodgkin's lymphoma (NHL) which itself consititue about $90 \%$ of all lymphomas.
A further molecular subclassfication of DLBCL into germinal centre B-cell-like (GCB) DBLCL and activated B-cell-like (ABC) DLCBL exist with very large differences at gene expression and molecular level.
The GCB cases exhibit a clinically favorable overall survial prognosis compared to that of ABCs.

\subsubsection{Data and preprocessing}
A large amount of DLBCL datasets are now available online at the NCBI (National Center for Biotechnology Information) Gene Expression Omnibus (GEO) website.
We downloaded 10 DLBCL large-scale gene expression studies and preprocessed these using the custom brainarray chip definition files (CDF) and RMA-normalization [7, 3].
The corresponding GEO-accession numbers are \Sexpr{paste(grep("XXX", base::unique(studies[,"GSE"]), invert = TRUE, value = TRUE), collapse = ", ")} of various microarray platforms.
The downloaded data together with our own yields a total of \Sexpr{rowSums(dlbcl.dims)["Samples"]} samples with studies sizes in the range \Sexpr{paste(range(dlbcl.dims["Samples",]), collapse = "-")}.
The summarization using brainarray CDFs facilitates cross-platform integration.

After the RMA normalization and summarization, the data was brought to a common scale by quantile normalizing all data to the common cumulative distribution function of all arrays.

Lastly, the datasets were reduced to the total of \Sexpr{dim(gep[[1]])["Features"]} common features represented in all studies and array platforms.

\subsubsection{Graphical meta-analysis}
We wanted to perform coexpression network (i.e.\ weighted correlation network) analysis across all datasets.
For each dataset the scatter matrix $\vS_i$ of the top \Sexpr{dlbcl.par$top.n} most variable genes (as measured by the pooled variance across studies) was computed as the suffcient statistics along with the number of samples.
Hence, we investigate \Sexpr{(dlbcl.par$top.n*(dlbcl.par$top.n + 1)/2)} pairwise interactions.

The parameters of the RCM model was estimated using the EM algorithm and yielded

<<Psi, echo=FALSE, fig.height=7/2, fig.width = 7>>=
r <- order(rowSums(abs(dlbcl.rcm$Psi)))
img <- dlbcl.rcm$Psi[r,r]
img <- img[ncol(img):1,]
par(mar = c(0,0,0,0))
n.cols <- 255
brks <- c(min(img),
          seq(quantile(img, 0.001), quantile(img, 1-0.01), l = n.cols - 1),
          max(img))
graphics::image(img, axes = FALSE, asp = 1, breaks = brks,
                col = colorRampPalette(c("red", "black", "blue"))(n.cols))
text(-0.05, 0.5, expression(hat(bold(Psi)) == phantom(a)))
@

\noindent and a $\hat{\nu} = \Sexpr{round(dlbcl.rcm[["nu"]], 2)}$.\fxnote{Interpretation of $\nu$? A good interpretation of $\nu$ would really add some value to the paper.}
Subsequently, $\hvPsi$ was scaled to the corresponding correlation matrix $\hat{\vec{R}}$.
Note, that the expected covariance $(\nu - p - 1)^{-1}\hvPsi$ yields the same correlation matrix $\hat{\vec{R}}$.
Figure \ref{fig:dlbcl_plot_1} shows a histogram of the distibution of correlations.

<<dlbcl_plot_1, echo = FALSE, fig.height=7/2.5, fig.width=7, fig.pos="tb", fig.scap = "", fig.cap="\\it Distribution of the correlation coefficents given by the entries of $\\vec{\\hat{R}}$.">>=
@
<<dlbcl_mappings, echo=FALSE, results='hide'>>=
@
<<dlbcl_clustering, echo=FALSE, results='hide'>>=
@
<<dlbcl_plot_2, echo = FALSE, message=FALSE, results='hide'>>=
@
\fig{\Sexpr{dlbcl_plot_2}}{0.8\textwidth}{\it{}Top left: dendrograms of the hierachical clustering, the identified modules, and the correlation matrix. Top right: the correlation network as laid out by the Fruchterman-Reingold algorithm using the edge-weights. The nodes are coloured after the identified modules with sizes proportional to their connectivity. The opacity of the edges are drawn as function of the edge weight. Bottom: A Hieriachical edge bundeling representation of the network. Negative and positive correlations are coloured blue and red, resp.
}

We have thus estimated a correlation matrix across all studies to which standard correlation network analyses can be employed.
To identify subgraphs, we used agglomerative hierachical clustering with \Sexpr{dlbcl.clust$method} linkage on the distance measure defined by 1 minus the absolute value of the correlation.
The tree was arbitrarily pruned at a height which yielded \Sexpr{length(unique(dlbcl.modules))} subgraphs (or, modules) coloured and named by colors.
Figure \ref{\Sexpr{dlbcl_plot_2}} show these results.
Table \ref{tab:dlbcl_mod_tab} show the genes within each module.

Next, the modules were screened for biological relevance using GO (Gene Ontology) enrichment analysis.
Table \ref{tab:GO_tabs} in Appendix \ref{sec:sig_go_term} show the signifcant GO-terms at signifcance level \Sexpr{dlbcl.par$go.alpha.level} for each module.
As seen, the GO-terms all appear highly relevant to the pathology of DLBCL.

<<dlbcl_go_analysis, echo=FALSE, results='hide'>>=
@
<<dlbcl_mod_tab, echo=FALSE, results='asis'>>=
@

Lastly, we checked if the identified modules were predictive of overall survival (OS) in the CHOP and R-CHOP-treated cohorts of the LLMPP dataset (GSE10846).
To do this, the eigengene for each module was computed and a multiple Cox proportional hazards model for OS was fitted with the module eigengenes as covariates.
For the most interesting modules, the Kaplan-Meier estimates were computed for groups arising when dichotomizing the values of corresponding eigengene as above or below the median value.
These results are seen in Figure \ref{fig:survival_analysis}.
The results further corroborates that the identified modules are biologically meaningfull and that RCM provide a reasonable estimate of the covariance.

<<survival_analysis, echo=FALSE, results='hide', fig.height=2.5*7*0.7, fig.width=2*7*0.7, fig.pos="tb", fig.scap = "", fig.cap="\\it The top row show $95\\%$ and $99\\%$ confidence intervals for the estimate of the Hazard ratio in the multiple Cox model. The bottom two rows show Kaplan-Meier estimates of the dichotomizing eigengenes.">>=
@

\clearpage

\subsection{Supervised classification}
<<discriminant_analysis, echo=FALSE, message=FALSE, results='hide'>>=
@
An quite different application of the RCM estimate can be found as a novel discriminant analysis.
As seen below, the estimates obtained can be utilized in supervised learning as a intermediate case of linear discriminant analysis (LDA) and quadratic discriminant analysis (QDA).x

Suppose $Y$ is a random variable denoting the class $1, ..., k$ and suppose $\vx$ is a random vector of the explanatory variables.
Recall that LDA and QDA finds the class $y$ by maximizing
\begin{align*}
  P(Y = y | \vX = \vx) =
    \frac{\pi_y f(\vx | Y = y)}
         {\sum_{y' = 1}^k \pi_{y'} f(\vx | Y = y')}
\end{align*}
where $\vx | Y = y$ is assumed to be $p$-dimensional gaussian distributed, i.e.\
\begin{align*}
  \vx | Y = y \sim \calN_p(\vec{\mu}_y, \vSigma_y).
\end{align*}
LDA differs from QDA by the additional assumption that $\vSigma = \vSigma_y$ for all classes $y$.
An intermediate classifier of LDA and QDA can thus be constructed by assuming the $\vSigma_y$'s are inversely Wishart distributed as in \eqref{eq:REM}, i.e., $\vSigma_y \sim \calW^{-1}(\vPsi, \nu)$, and using the estimates of a common $\vSigma$ discussed above.
This hierarchical discriminant analysis (HDA) is thus straight-forward to implement given that
\begin{align*}
f(\vx | Y = y)
  &= \int f(\vx| \vSigma_y, Y = y) f(\vSigma_y | Y = y) d\vSigma_y \\
  &= \frac{ |\vPsi|^{\frac{\nu}{2}} \Gamma_p\!\left(\frac{\nu + 1}{2}\right) }
          { \pi^{-\frac{n}{2}} |\vPsi + (\vx-\mu_k)(\vx-\mu_k)^\top|^{\frac{\nu + 1}{2}}
            \Gamma_p\!\left(\frac{\nu}{2}\right)},
\end{align*}
analogous to the derivation in Appendix \ref{sec:marginalization}. The matrix determinant lemma, $|\vec{A} + \vec{u}\vec{v}^\top| = (1 + \vec{v}^\top\vec{A}\vec{u})|\vec{A}|$, can then be applied to simplify the expression and speed up the computations \citep{Ding2007}.
Note also that HDA generalizes the LDA in some sense to have a multivariate $t$-like distribution. Multivariate $t$-distributions has before considered for discriminant analysis, cf.\ \citet{Andrews2011}. \fxnote{Elaboration on the multivariate $t$-distribution look and reference!}


\subsubsection{Benchmarking of HDA}
We designed four different scenarios to test and identify where HDA can be expected perform similar, better, or worse compared to LDA and QDA as gauged by the misclassification risk. The simulation experiment was inspired by the one seen in \citet{Friedman1989}.

In all four scanarios, we generated a training dataset of a total of $n = \Sexpr{par.xda[["n.obs"]]}$ observations belonging to $k = \Sexpr{par.xda[["K"]]}$ classes.
First, class labels was generated from a multinomial distribution with equal probabilities for each class, $\pi_1 = \pi_2 = \pi_3 = 1/3$.
Hence, in each simulation round $\Sexpr{round(par.xda[["n.obs"]]/par.xda[["K"]], 2)}$ observations was expected in each group.
Conditional on the class the observations where drawn i.i.d.\ from a multivariate gaussian distribution, i.e.\ $x_{i}|K = k \sim \calN_p(\vmu_k, \vSigma_k)$.
The four scenarios of consisted of different choices, described later, of covariance matrices $\vSigma_k$ and mean values $\vmu_k$ for each class.

The \Sexpr{par.xda[["K"]]} covariance matrices were chosen to either be (a) equal and spherical, (b) unqual and spherical, (c) equal and highly elliptical, and (d) unequal and highly elliptical.
In scenario (a), $\vSigma_1 = \vSigma_2 = \vSigma_3 = \vI$.
In scenario (b), $\vSigma_k = k \vI$.
In scenario (c), the covariance matrices equal, $\vSigma_1 = \vSigma_2 = \vSigma_3$, and chosen such that the squareroot of the $d$ eigenvalues are equidistant on the interval 10 to 1 and a randomly (uniformly) oriented orthonormal basis is used for all components \citep{Friedman1989}.
In scenario (d), the eigenvalues are chosen as in scenario (c) for all $\vSigma_k$ but the orientation of the orthonormal basis of eigenvectors differs.

In all scenarios expect (c) the mean values were chosen to $\vec{\mu}_1 = \vec{0}, \vec{\mu}_2 = 3\vec{e}_1, \vec{\mu}_3 = 4\vec{e}_2$ where $\vec{e}_j$ denotes the $j$th basis-vector.
In scenario (c), $\vec{\mu}_1 = \vec{0}$ and the remaning means values are chosen such that the differences project mainly onto the low-variance subspace

Each of the four scenarios were examined with varying dimensionality $p = \Sexpr{paste(par.xda[["p.dims"]], collapse = ", ")}$.
Using the described parameters a training and validation set each of $\Sexpr{par.xda[["n.obs"]]}$ and $\Sexpr{par.xda[["n.obs.valid"]]}$ observations, respectively, observations was generated.
For each method the classifier was trained on the training data followed by classification of the validation data and computation of the misclassification risk.
Each simulation setup was repeated $\Sexpr{par.xda[["n.runs"]]}$ times and the mean and standard deviation of the misclassification risks were computed.
Table \ref{HDA_tab} shows the results of the simulation experiments.
\providecommand{\red}{\color{red}}
\definecolor{mygreen}{rgb}{0,0.6,0}
\providecommand{\green}{\color{mygreen}}
<<hda_table_res, echo=FALSE, results='asis'>>=
@
In cases where the sample covariance matrix is not invertible, a small constant was added to the diagonal to allow for stable inversion similar to \citet{Friedman1989}.

In the equal and spherical case (a), HDA yields almost identical results to LDA which both unsurprisingly outperform QDA.
Also perhaps expected, the difference between LDA and QDA is less prominent for low dimensional spaces.
The same holds true for the unqual and spherical case (b) and (c). Here HDA and LDA perform somewhat equally well, while HDA is

Most interestingly, HDA is seen to practically always perform at least as good as LDA in all scenarios.
HDA is also consistently superior for the large dimensional tests.
The largest gain from HDA to LDA was seen in the high-dimensional scenario (c).

This demonstrate that HDA as potentially useful addition to the descriminant analysis toolbox.
However, we recognize that further and more sophisticated simulation experiments are needed  to explore the scenarios where HDA should be considered a serious alternative.






\section{Concluding remarks}
The article provides a basic framework and attempt for modeling a common covariance structure across multiple groups or datasets.
The straight-forward approaches, of using the mean or pooled covariance matrix, are seen as a moment estimators within the model.
The estimate using the EM algorithm is shown to be superior to the simple alternatives.
While the improvements are modest, the article demonstrate an advantage of modelling the inter-study variance as a hierarchical random effects model.
However, the virtue of such a model is not from improvement in accuracy alone.
Also desirable is the explicit quantification of the inter-study variance.
If $\hat{\nu}$ is estimated to be large, the studies exhibit a largely common covariance structure, and vice-versa when $\hat{\nu}$ is small.

As demonstated, combining multiple studies can yield a sufficiently large total sample size that allows for estimation of large covariance matrices without the use of regularization
While the generalization of the model to $n \gg p$ is extremely interesting though out of scope for this article.
We believe this work could be furhter enriched by combining the method with regularized estimation.

The recent advances in such regularized techiniques which allows for analysis for very large covariance matrices has unfortunately diminished the focus on collecting larger sample.
Just because it is technically possible, it does not mean a good estimate is achived.
For example, while non-zero entries often can be accurately recalled in graphical LASSO, actual estimates of the covariances (or precisions) can still be heavily biased.
Large sample-sizes are still needed achive unbiased estimates of the covariance.
Therefore, an increased focus should also be appointed to efficiently aggregating datasets and achiving suffciently large sample sizes to allow for stable and unbiased estimation of covariance matrices.

One might question the added ultiliy of the $\nu$ parameter is an obvious critique of the RCM.
For example, it is unclear how large a proportion a single extra parameter can explain of the interstudy variance.
Though more parameters could be introduced, we have attempted to provide some intuitive interpretation of the $\nu$ parameter.
\fxnote{Do we have any other observations?}










\newpage
\bibliographystyle{plainnat}
\bibliography{references_mendelay,references_manual}


\newpage
\appendix






\section[Marginalization of Sigma]{Marginalization of $\vSigma$}
\label{sec:marginalization}
This sections shows the marginalization out of $\vSigma$ in \eqref{eq:loglik}. For ease of notation we drop the subscript $i$ used in $\vSigma_i$, $\vX_i$, $\vS_i = \vX_i \vX_i^\top$, and $n_i$ in the above text. We do the computation straight-forwardly by the assumptions of the model,
\small
\begin{align*}
  &f(\vX | \vPsi, \nu) \\
  &= \int f(\vX|\vSigma) f(\vSigma | \vPsi, \nu) d\vSigma \\
  &= \int \left[ \prod_{j = 1}^n  (2\pi)^{-\frac{p}{2}} |\vSigma|^{-\frac{1}{2}}
                         e^{-\frac{1}{2}\tr(\vx_{ij}\vx_{ij}^\top\vSigma^{-1})} \right]
          \frac{|\vPsi|^{\frac{\nu}{2}}}{2^{\frac{\nu p}{2}}\Gamma_p\!\left(\frac{\nu}{2}\right)}
          |\vSigma|^{-\frac{\nu+p+1}{2}}e^{-\frac{1}{2}\tr(\vPsi\vSigma^{-1})}
      \;d\vSigma \\
  &= (2\pi)^{-\frac{np}{2}}
      \frac{|\vPsi|^{\frac{\nu}{2}}}{2^{\frac{\nu p}{2}}\Gamma_p\!\left(\frac{\nu}{2}\right)}
      \int
        |\vSigma|^{-\frac{n}{2}}  e^{-\frac{1}{2}\tr(\vS\vSigma^{-1})}
        |\vSigma|^{-\frac{\nu+p+1}{2}} e^{-\frac{1}{2}\tr(\vPsi\vSigma^{-1})}
      \;d\vSigma \\
  &=
      \frac{|\vPsi|^{\frac{\nu}{2}}}{\pi^{-\frac{np}{2}}2^{\frac{(\nu + n) p}{2}}\Gamma_p\!\left(\frac{\nu}{2}\right)}
      \int
        |\vSigma|^{-\frac{(\nu + n)+p+1}{2}}
         e^{-\frac{1}{2}\tr\!\left((\vPsi+ \vS)\vSigma^{-1}\right)}
      \;d\vSigma.
\end{align*}
\normalsize
The integrand here can be recognized as a unnormalized inverse Wishart pdf,  $\calW^{-1}(\vPsi + \vS, \nu + n)$, and so the integral evaluates to the reciprocal value of the normalizing constant in that density. Thus,
\begin{align*}
  f(\vX | \vPsi, \nu)
  &=
    \frac{|\vPsi|^{\frac{\nu}{2}}}
         {\pi^{-\frac{np}{2}} 2^{\frac{(\nu + n) p}{2} } \Gamma_p\!\left(\frac{\nu}{2}\right)}
    \frac{2^\frac{(v+n)p}{2} \Gamma\left(\frac{\nu + n}{2}\right)}
         {|\vPsi + \vS|^{\frac{\nu + n}{2}}} \\
  &=
    \frac{|\vPsi|^\frac{\nu}{2} \Gamma\left(\frac{\nu + n}{2}\right)}
         {\pi^{-\frac{np}{2}} |\vPsi + \vS|^{\frac{\nu + n}{2}}\Gamma_p\!\left(\frac{\nu}{2}\right)},
\end{align*}
which was as wanted.








\section{Non-concavity of the log-likelihood}
\label{sec:concaveloglik}
The likelihood function is not log-concave in general.
This section analyse the (non)-concavity of the log-likelihood function,
\small
\begin{align}
  &\ell\!\left(\vPsi, \nu \big| \vX_1, ..., \vX_k \right) \notag \\
  &= c + \sum_{i=1}^k \bigg[
            \frac{\nu}{2}  \log |\vPsi|
            + \log \Gamma_p\!\left(\frac{\nu + n_i}{2}\right)  %\notag\\
           - \frac{\nu + n_i}{2}\log|\vPsi +\vS_i|
            - \log \Gamma_p\!\left(\frac{\nu}{2}\right)
            \bigg].
            \label{eq:appendloglik}
\end{align}
\normalsize
More precisely, the following two propositions are proved.
\begin{proposition}[Non-concavity in $\vPsi$]
\label{prop:nonconcavityinpsi}
For fixed $\nu$, the log-likelihood function \eqref{eq:appendloglik} is not concave in $\vPsi$ (and hence not in general).
\end{proposition}
\begin{proposition}[Concavity in $\nu$]
\label{prop:concavityinnu}
For fixed $\vPsi \succeq 0$, the log-likelihood function \eqref{eq:appendloglik} is concave in $\nu$.
\end{proposition}

\begin{proof}[Proof of Proposition \ref{prop:nonconcavityinpsi}]
Assume $\nu$ fixed and consider the terms involving $\vPsi$ in \eqref{eq:appendloglik}. Not counting the constant, the first term, $\frac{\nu}{2}  \log | \vPsi |$, is concave by the well known result that the log-determinant of positive semi-definite matrix is concave \citep[See e.g.][pp. 73-74]{Boyd2004}.
The third term in \eqref{eq:appendloglik}, $- \frac{\nu + n_i}{2}\log|\vPsi + \vS_i|$, is convex by the same argument however negated by the sign.
The sum $\vPsi +\vS_i$ is positive semi-definite since both summands are.
The concavity is not clear from this.

We reduced to the one-dimensional case where
\begin{align*}
  \ell'(\psi)
  = \frac{k\nu}{2}\frac{1}{\psi} +
      \sum_{i = 1}^k \frac{\nu + n_i}{2}\frac{1}{\psi + x_i^2}
\end{align*}
which is not clearly convex. We see, that
\begin{align*}
  \lim_{\psi \to 0} \ell'(\psi) = \infty
  \text{  and  }
  \lim_{\psi \to \infty} \ell'(\psi) = 0
\end{align*}
We draw the likelihood and its derivative with $k = 1$ and other appropriately chosen parameters as seen in Figure \ref{fig:one_dimensional_loglik} generated by
<<one_dimensional_loglik, echo=-3, fig.height=3, fig.width=6, fig.pos="tb", ig.scap = "", fig.cap = "\\it One-dimensional log-likelihood and its the derivative.">>=
@
Clearly, the log-likelihood $\ell$ is not log-concave since a differentiable function of one variable is concave if and only if its derivative is monotonically non-increasing.
A unique root, however, is seen.
\end{proof}

\begin{proof}[Proof of Proposition \ref{prop:concavityinnu}]
Consider the terms involving $\nu$.
Clearly, the mixed terms involving both $\nu$ and $\vPsi$ are log-linear in $\nu$ and hence log-concave.
We thus restrict our attention to the remaining terms not dependent on $\vPsi$.
By themselves, the second term in \eqref{eq:appendloglik},
$
\log \Gamma_p\!\left(\frac{\nu + n_i}{2}\right)
$
is convex since the multivariate gamma function is log-convex, cf.\ section \ref{sec:multigamma}.
In the same manner, the fourth term,
$
  - \log \Gamma_p\!\left(\frac{\nu}{2}\right),
$
is concave by the negative sign.
The sum however of these terms involving $\Gamma_p$ are concave in $\nu$, since
\begin{align*}
  &\log\Gamma_p\!\left( \frac{\nu + n_i}{2} \right) -
    \log\Gamma_p\!\left( \frac{\nu}{2} \right)
  =  \log\frac{\Gamma_p\!\left( \frac{\nu + n_i}{2} \right)}{
                \Gamma_p\!\left( \frac{\nu}{2}       \right)}
  = \log \prod_{j = 1}^p
    \frac{\Gamma\!\left( \frac{\nu + 1 - j}{2} + \frac{n_i}{2} \right)}{
          \Gamma\!\left( \frac{\nu + 1 - j}{2} \right)}.
\end{align*}
which can be seen to be concave since $n_i \geq 2$ for all $i$ and
\begin{align}
  x \mapsto \log\left(\frac{\Gamma(x + a)}{\Gamma(x)}\right)
  \label{eq:logGammaRatio}
\end{align}
is concave for all $x>0$ and $a > 0$.
While we do not provide a proof of this the mapping is plotted in Figure \ref{fig:log_gamma_ratio}.
Hence, the log-likelihood is log-concave in $\nu$.
<<log_gamma_ratio, echo=-3, fig.height=4.5/2, fig.width=4.5, fig.pos="ht", fig.scap = "", fig.cap="\\it Plots of the mapping given in \\eqref{eq:logGammaRatio} for diffent values of $a$.">>=
@
\end{proof}






\subsection{log-convexity of the multivariate gamma function}
\label{sec:multigamma}
The log-convexity of the multivariate gamma function can seen using the following characterization of $\Gamma_p$,
\begin{align}
  \label{eq:multigamma}
  \Gamma_p(t) = \pi^{ \frac{1}{2} \binom{p}{2} }
  \prod_{j = 1}^p \Gamma\!\left(t + \frac{1 - j}{2}\right)
  \text{ where }
  \Gamma(t) = \int_0^\infty x^{t-1} e^{-x} dx.
\end{align}
From this
\begin{align}
  \label{eq:logmultigamma}
  \log\Gamma_p(t) =
  \frac{1}{2}\binom{p}{2} \log\pi +
  \sum_{j = 1}^p \Gamma\left(t + \frac{1-j}{2}\right),
\end{align}
which is convex since $\Gamma$ is log-convex and sums of convex functions are convex.
Hence $\Gamma_p$ is log-convex.





\section{Uniqueness of log-likelihood maxima}
\label{sec:negativedefinite}
This section proves the following lemmas which imply the following proposition.

\begin{lemma}
\label{lem:elltominusinfty}
If $\nu$ is fixed and $n_\bullet = \sum_{a=1}^k n_a \geq p$ then the log-likelihood $\ell(\vPsi) \to -\infty$ whenever $\exists i : \lambda_i \to 0 \vee \lambda_i \to \infty$, where $\lambda_i$ is a eigenvalue of $\vPsi$.
\end{lemma}

\begin{lemma}
\label{lem:negativesemidefinite}
The hessian of the log-likelihood \eqref{eq:appendloglik} is negative semi-definite in all stationary points for fixed $\nu$.
\end{lemma}

\begin{proposition}[Existence and uniqueness of maximum]
\label{prop:uniquemax}
The log-likelihood \eqref{eq:appendloglik} has a unique maximum in $\vPsi$ for fixed $\nu$.
\end{proposition}

\noindent The log-likelihood in \eqref{eq:loglik}, assuming $\nu$ fixed, obey
\begin{align}
  2\ell(\vPsi)
  = k\nu\log|\vPsi| - \sum_{a=1}^k (n_a + \nu)\log|\vPsi + \vS_a|
\label{eq:loglik2}
\end{align}
up to addition of a constant.

\begin{proof}[Proof of Proposition \ref{prop:uniquemax}]
The proposition follows from Lemma \ref{lem:elltominusinfty} and
Lemma \ref{lem:negativesemidefinite}.
First, we prove existenece.
By Lemma \ref{lem:elltominusinfty}, the set
\begin{align*}
  \{ \vPsi | \ell(\Psi) > \ell(\vPsi^*) \}
\end{align*}
is compact for any $\vPsi^*\succ 0$ since it is bounded and closed by the virue of the Lemma and continuity of $\ell$, respectively.
The rest follows from Rolle's theorem.

\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:elltominusinfty}]
Assume the hypothesis of the lemma and consider the expression given in \eqref{eq:loglik2}.
If $\lambda_i \to \infty$ then
\begin{align*}
  \ell(\vPsi)
  &= \frac{k\nu}{2}\log|\vPsi| - \sum_{i = 1}^k \frac{\nu + n_i}{2} \log |\vPsi+\vS_i| \\
  &\leq \frac{k\nu}{2}\log|\vPsi| - \sum_{i = 1}^k \frac{\nu + n_i}{2} \log |\vPsi|
  =  - \frac{n_\bullet}{2} \log |\vPsi| \to -\infty.
\end{align*}
This proves the first case where the largest eigenvalue diverge to infinity.
Suppose $\lambda_i \to 0$ and let
\begin{align*}
  C = \sum_{i = 1}^k \frac{(\nu + n_i)}{2} = \frac{k\nu}{2} + \frac{n_\bullet}{2},
\end{align*}
then \eqref{eq:loglik2} can be expressed as
\begin{align*}
  \ell(\vPsi)
  = \frac{k\nu}{2}\log|\vPsi| -
    C \sum_{i = 1}^k \frac{(\nu + n_i)}{2C} \log |\vPsi+\vS_i|.
\end{align*}
Since $\log|\cdot|$ is concave and the above sum is a convex combination, we have
\begin{align*}
  \ell(\vPsi)
  \leq \frac{k\nu}{2}\log|\vPsi| -
     C \log\left| \vPsi + \sum_{i = 1}^k \frac{(\nu + n_i)}{2C}\vS_i\right|.
\end{align*}
Clearly, the first term goes to $-\infty$ whenever an eigenvalue $\lambda_i \to 0$.
The matrix in the second term is almost surely positive definite since $\sum_{a=1}^k x_a \geq p$ and the log determinant will converge to some constant.
Hence, if $\lambda_i \to 0$ then
\begin{align*}
  \ell(\vPsi) \leq \frac{k\nu}{2}\log|\vPsi| + C_2 \to -\infty,
\end{align*}
which completes the proof.
\end{proof}

\begin{proof}[Proof of Lemma \ref{lem:negativesemidefinite}]
The matrix cookbook by \citet{Petersen2008} has been a useful reference here. See equations (41, p.\ 8) and (59, p.\ 9) and pages 14 and 52-53 in \citep{Petersen2008}. We first compute expression for the 1.\ and 2.\ order derivatives.

\subsubsection*{1.\ order derivatives}
From the log-likelihood expression, we compute the 1.\ order derivative $\nabla_\vPsi 2\ell(\vPsi)$ which is the matrix-valued function where each entry is given by
\begin{align}
  %\left(\frac{\partial 2\ell}{\partial \vPsi}\right)_{ij}=
  \frac{\partial 2\ell}{\partial \Psi_{ij}}
  = k\nu\tr\!\left(\vE^{ij}\vPsi^{-1}\right)
    - \sum_{a = 1}^k (\nu + n_a)\tr\!\left(\vE^{ij}\left(\vPsi + \vS_a\right)^{-1}\right).
\label{eq:dloglik}
\end{align}
where $\vE^{ij}$ is a matrix with ones at entries $(i,j)$ and $(j,i)$ and zeros elsewhere.
This $\vE^{ij}$ is introduced as the derivative is not straight-forward because of the symmetric structure of $\vPsi$. Had $\vPsi$ been unstructured, then $\frac{\partial}{\partial \vPsi}\log|\vPsi| = \vPsi^{-1}$.
However, when $\vPsi$ is symmetric we have that $\frac{\partial}{\partial \Psi_{ij}}\log|\vPsi| = \tr(\vE^{ij}\vPsi^{-1})$ which is to say $\frac{\partial}{\partial \vPsi}\log|\vPsi| = 2\vPsi^{-1} -\vPsi^{-1} \circ \vI$ where $\circ$ denotes the Hadamard product \citep[eq.\ (43) and (141)]{Petersen2008}.

The first order derivative lives in a $\binom{p+1}{2}$-dimensional vector space indexed by $(i,j)$, $i\leq j$, with basis vectors $\vE^{ij}$.




\subsubsection*{2.\ order derivatives}
We proceed with the second order derivative $\nabla^2_\vPsi 2\ell(\vPsi)$ with entries given by
\begin{align*}
  \frac{\partial^2 2\ell}{\partial \Psi_{kl} \partial \Psi_{ij}}
  &= - k\nu\tr\!\left( \vE^{ij}\vPsi^{-1} \vE^{kl}\vPsi^{-1} \right) \\
  & + \sum_{a = 1}^k (\nu + n_a)
    \tr\!\left(
      \vE^{ij}\left(\vPsi + \vS_a\right)^{-1}
      \vE^{kl}\left(\vPsi + \vS_a\right)^{-1}
    \right),
\end{align*}
obtained by differentiation of \eqref{eq:dloglik} combined with
$\frac{\partial}{\partial \Psi_{ij}} \vPsi^{-1} = - \vPsi^{-1}\vE^{ij}\vPsi^{-1}$ \citep[eq.\ (40)]{Petersen2008} and the linearity of the trace operator.

The second order derivative is a $\binom{p+1}{2} \times \binom{p+1}{2}$-dimensional matrix indexed by $(i,j)$ and $(k,l)$, $i \leq j$, $k \leq l$.




\subsubsection*{Negative semi-definiteness in stationary points}
With the above expressions we now show that the hessian is negative semi-definite in all stationary points (or, extrema).
Let $\vY = \sum_{(i,j)} y_{ij}E_{ij}$ be a symmetric matrix in the vector space where $\vY \neq \vec{0}$. The analog to $\vec{y}^\top \vec{A}\vec{y} = y_i \sum_{ij} A_{ij} y_j \leq 0$ in our vector space then becomes
\begin{align*}
  \sum_{i\leq j, k\leq j}
    Y_{ij}
    \left(\nabla^2_\vPsi 2\ell(\vPsi)\right)_{(i,j),(k,l)}
    Y_{kl} \leq 0
\end{align*}
which amounts to showing that
\small
\begin{align}
- k\nu\tr\!\left( \vY\vPsi^{-1} \vY\vPsi^{-1} \right)
+ \sum_{a = 1}^k (\nu + n_a)
    \tr\!\left(
      \vY\left(\vPsi + \vS_a\right)^{-1}
      \vY\left(\vPsi + \vS_a\right)^{-1}
    \right) \leq 0.
  \label{eq:negativedefinte}
\end{align}
\normalsize
Now, the positive-definiteness of $\vPsi$, let
\begin{align*}
  \vY &:= \vPsi^{-\frac{1}{2}} \vY \vPsi^{-\frac{1}{2}} \text{ and } \\
  \vS_a &:= \vPsi^{-\frac{1}{2}} \vS_a  \vPsi^{-\frac{1}{2}}.
\end{align*}
we can assume that $\vPsi = \vI$. Hence, the likelihood equation \eqref{eq:loglik2} equated to zero, becomes
\begin{align*}
  k\nu\vI = \sum_a(n_a + \nu)(\vI + \vS_a)
\end{align*}
which implies (by multiplication by $\vY^2$)
\begin{align}
  k\nu\tr(\vY^2)
  &= \sum_a (n_a + \nu)\tr\!\left(\vY^2(\vI + \vS_a)\right) \notag\\
  &= \sum_a (n_a + \nu)\tr\!\left(\vY(\vI + \vS_a)\vY\right).
  \label{eq:loglikequation}
\end{align}
We substitute \eqref{eq:loglikequation} into \eqref{eq:negativedefinte} to get
\begin{align*}
  &\sum_a (n_a + \nu)\tr\left(\vY (\vI + \vS_a) \vY (\vI + \vS_a) - \vY (\vI + \vS_a) \vY \right) \\
  &=  \sum_a (n_a + \nu)\tr\!\Big( \vY (\vI + \vS_a) \vY \big[ (\vI + \vS_a)  - \vI \big]\Big)
  \leq 0
\end{align*}
Since $\vS_a$ is positive semi-definite, it can be diagonalized $\vS_a = \vU_a \vD_a \vU_a^\top$ by the spectral theorem where the diagonal matrix $\vD_a \succeq 0$ (all non-zero entries are positive) and $\vU_a$ is orthonormal (i.e.\ $\vU_a\vU_a^\top = \vU_a^\top\vU_a = \vI$). Using the diagonalization,
\begin{align}
  \sum_a (n_a + \nu)\tr\!\Big(
    \underbrace{\vU^\top \vY (\vI + \vS_a)^{-1} \vY \vU}
    \big[ (\vI + \vD_a)^{-1}  - \vI \big]
  \Big)
  \leq 0
  \label{eq:loglikequation2}
\end{align}
where the underbraced matrix is positive semi-definite and hence have non-negative diagonal elements.
Furthermore, since $\vD_a$ is diagonal also with non-negative elements, the diagonal matrix $(\vI + \vD_a)^{-1}  - \vI$ clearly have non-positive entries and is thus negative semi-definite.

Since the trace of a matrix product is the sum of the element-wise products, the trace (and thus the sum) will always be non-positive and hence \eqref{eq:loglikequation2} will always hold.
\end{proof}



\section{log-likelihood of the precision}
\label{sec:precisionloglik}
Suppose we have $k$ i.i.d. realizations, $\vDelta_1, ..., \vDelta_k$, from the Wishart distribution given in equation \eqref{eq:precisiondensity}. The corresponding log-likelihood can be computed straight-forwardly:
\begin{align*}
  \ell(\vTheta | \vDelta_1, ..., \vDelta_k)
  &= \sum_{i = 1}^k \log f(\vDelta_i | \vTheta) \\
  &= \sum_{i = 1}^k \log
    \frac{|\vTheta|^{-\frac{\nu}{2}}}{
      2^{-\frac{vp}{2}}
      \Gamma_p\!\left(\frac{\nu}{2}\right)}
    |\vDelta_i|^\frac{\nu - p - 1}{2}e^{-\frac{1}{2}\tr(\vTheta^{-1}\vDelta_i)}\\
   &= c + \sum_{i = 1}^k
     -\frac{\nu}{2} \log |\vTheta|
     -\frac{1}{2}\tr(\vTheta^{-1}\vDelta_i) \\
   &= c + \frac{\nu k}{2}
     \left(
       \log |\vTheta| +
       \tr\!\left(\vTheta^{-1} \frac{1}{\nu k}\sum_{i = 1}^k\vDelta_i\right)
     \right).
\end{align*}
The last expression is to be maximized with respect to $\vTheta$ and can be recognized as the MLE problem in a multivariate Gaussian distribution. Hence,
\begin{align*}
  \vTheta = \frac{1}{k \nu} \sum_{i = 1}^k \vDelta_i,
\end{align*}
is the MLE in this model.


\section{Model and notation overview}
Let $\vDelta_i = \vSigma_i^{-1}$, $\vTheta = \vPsi^{-1}$, and $\vS_i = \vX_i^\top \vX_i$. Then the following equivalences hold.
\begin{equation*}
\begin{aligned}
  \vSigma_i &\sim \calW^{-1}_p(\vPsi, \nu) \\
  \vX_i | \vSigma_i &\sim \calN_p(\vec{0}_p, \vSigma_i) \\
  &\Updownarrow \\
  \vSigma_i | X_i  &\sim \calW^{-1}_p(\vPsi + \vS_i, \nu + n_i)
\end{aligned}
\begin{aligned}
\quad
&\Longleftrightarrow \\
&\Longleftrightarrow \\
\\
&\Longleftrightarrow
\quad
\end{aligned}
\begin{aligned}
  \vDelta_i &\sim \calW_p(\Theta, \nu) \\
  \vX_i | \vDelta_i &\sim \calN_p(\vec{0}_p, \vDelta_i^{-1}) \\
  &\Updownarrow \\
  \vDelta_i | X_i  &\sim \calW_p((\vTheta^{-1} + \vS_i)^{-1}, \nu + n_i)
\end{aligned}
\end{equation*}


\section{Module member genes}
\label{sec:module_member_genes}
<<>>=
print(lapply(dlbcl.module.genes, unname))
@

\section{Signifcant GO terms}
\label{sec:sig_go_term}
Tables of the significant GO terms.
<<GO_tabs, echo=FALSE, results='asis'>>=
@
\clearpage


\end{document}

